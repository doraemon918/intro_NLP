{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP Class\n",
    "\n",
    "from pipeUtils import Document\n",
    "from pipeUtils import Annotation\n",
    "import re\n",
    "\n",
    "class PadClassificationSystem:\n",
    "    def __init__(self):\n",
    "        #initiate necessary components        \n",
    "        self.target_rules=self.getTargetRegexes()        \n",
    "        self.negation_rules = self.getNegRegexes()\n",
    "                \n",
    "    def process(self, document):\n",
    "        document_id = document.document_id\n",
    "        ann_index=0\n",
    "        for reg in self.target_rules:\n",
    "            for match in reg.finditer(document.text):\n",
    "                ann_id = 'NLP_'+ str(document_id) + '_' + str(ann_index)\n",
    "                ann_index=ann_index+1\n",
    "                new_annotation = Annotation(start_index=int(match.start()), \n",
    "                                    end_index=int(match.end()), \n",
    "                                    type='pad_annotation',\n",
    "                                    ann_id = ann_id\n",
    "                                    )\n",
    "                new_annotation.spanned_text = document.text[new_annotation.start_index:new_annotation.end_index]\n",
    "\n",
    "                # Check negation right before the found target up to 30 charachers before, \n",
    "                # making sure that the pre-text does not cross the text boundary and is valid\n",
    "\n",
    "                if new_annotation.start_index - 30 > 0:\n",
    "                    pre_text_start = new_annotation.start_index - 30\n",
    "                else:\n",
    "                    pre_text_start = 0\n",
    "\n",
    "                # ending index of the pre_text is the beginning of the found target    \n",
    "                pre_text_end = new_annotation.start_index    \n",
    "\n",
    "                # substring the document text to identify the pre_text string\n",
    "                pre_text = doc.text[pre_text_start: pre_text_end]\n",
    "\n",
    "                # We do not need to know the exact location of the negation keyword, so re.search is acceptable\n",
    "                for neg_regex in self.negation_rules:\n",
    "                    if re.search(neg_regex, pre_text):\n",
    "                        new_annotation.attributes[\"Negation\"] =\"Negated\"\n",
    "\n",
    "                document.annotations.append(new_annotation)\n",
    "        \n",
    "        return document \n",
    "    \n",
    "    def getTargetRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'(peripheral\\s*(arter\\w*|vasc\\w*)\\s*disease)',\n",
    "            r'\\bpvd\\b',\n",
    "            r'femoral.{1,50}occlusion'\n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes\n",
    "\n",
    "    def getNegRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'\\bno\\b',\n",
    "            r'no\\s*evidence\\s*of'  ,\n",
    "            r'does\\s*not\\s*have',\n",
    "            r'denies'\n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  test case\n",
    "nlp_system = PadClassificationSystem()\n",
    "doc_text = '''\n",
    "Patient has peripheral artery disease. ---------- \\nPatient also has PVD or peripheral vascular\\ndisease or pvd . \n",
    "\\n The patient does not have any peripheral artery disease \n",
    "but has peripheral arterial disease . The patient denies having peripheral vascular disease . \\n \n",
    "The patient has a femoral and illiac occlusion which is suggestive of peripheral arterial disease.\n",
    "'''\n",
    "doc=Document(text=doc_text, document_id='Doc1')\n",
    " \n",
    "out_doc=nlp_system.process(doc)\n",
    "print(out_doc.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeUtils import Annotation\n",
    "from pipeUtils import Document\n",
    " \n",
    "import os\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all test documents\n",
    "unid=\"YOUR_UNID\"\n",
    "project_1 = \"PAD_ann_train\"\n",
    "project_2 = \"PAD_ann_test\"\n",
    "path_1 = \"/home/\"+str(unid)+\"/BRAT/\"+str(unid)+\"/\"+project_1\n",
    "path_2 = \"/home/\"+str(unid)+\"/BRAT/\"+str(unid)+\"/\"+project_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs=dict()\n",
    "test_doc_paths = glob.glob(str(path_2+'/*.txt')) \n",
    "for d in test_doc_paths:\n",
    "    doc = Document()\n",
    "    #print(d)\n",
    "    doc.load_document_from_file(d)\n",
    "    #print(str(d[:-3])+'ann')\n",
    "    doc.load_annotations_from_brat(str(d[:-3])+'ann')\n",
    "    #print(os.path.basename(d))\n",
    "    test_docs[os.path.basename(d)]=doc\n",
    "\n",
    "\n",
    "test_docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the all notes\n",
    "nlp_system = PadClassificationSystem()\n",
    "\n",
    "for doc_id in  test_docs.keys():\n",
    "    nlp_system.process(test_docs.get(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).compare_types_by_span('PAD','pad_annotation', False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "for a in tp_list_total:\n",
    "    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "attributes_to_compare=[]\n",
    "# To compare attributes, create a list of tuples for each pair to compare:\n",
    "# attributes_to_compare.append[(A1_type, A1_att_name, A1_att_value),(A2_type, A2_att_name, A2_att_value)]\n",
    "attributes_to_compare.append([('PAD', 'Negation', 'Negated'),('pad_annotation', 'Negation', 'Negated')])\n",
    "\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).\\\n",
    "    compare_types_by_span_and_attributes('PAD','pad_annotation', attributes_to_compare , False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "#for a in tp_list_total:\n",
    "#    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),\n",
    "                       db='mimic2')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify patients with PAD for reference standard\n",
    "pad_data = pd.read_sql(\"\"\"SELECT noteevents.subject_id, \n",
    "                      noteevents.category, \n",
    "                      noteevents.text FROM noteevents limit 100000 \"\"\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn.close()\n",
    "except:\n",
    "    print(\"Connection is already closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pad_data.columns)\n",
    "print(\"Number of records = \", len(pad_data))\n",
    "\n",
    "pad_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nlp_system = PadClassificationSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "counter = 0\n",
    "for index , row in pad_data.sample(10000).iterrows():    \n",
    "    doc = Document(document_id=str(row.subject_id) + '_' + str(index), text=row.text)\n",
    "    final_nlp_system.process(doc)\n",
    "    if(len(doc.annotations) > 0):\n",
    "        i = 1\n",
    "        for a in doc.annotations:\n",
    "            if( a.type == 'pad_annotation'):\n",
    "                neg_flag = 0\n",
    "                # Switch the flag to 1 when the mention is negated\n",
    "                if('definite_negated_existence' in a.attributes):\n",
    "                    neg_flag=1\n",
    "                ### Each row in the dictionary\n",
    "                record_id  = str(row.subject_id) + '_' + str(index)+'_'+str(i)\n",
    "                subject_id =  row.subject_id\n",
    "                note_id = str(row.subject_id) + '_' + str(index)\n",
    "                annotation_type = a.type\n",
    "                snippet = doc.text[int(a.start_index): int(a.end_index)]\n",
    "                out_list = [record_id, subject_id, note_id, annotation_type, \\\n",
    "                            a.start_index, a.end_index, \\\n",
    "                            snippet, neg_flag]\n",
    "                output.append(out_list)\n",
    "                i=i+1\n",
    "                counter=counter+1\n",
    "                # Print . after 10 identified records\n",
    "                if counter%10 == 0:\n",
    "                    print('.', end='')\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['record_id','subject_id', 'note_id', 'annotation_type', 'span_start', 'span_end', 'PAD_snippet', 'neg_flag']\n",
    "result_data_frame = (pd.DataFrame(output, columns=columns))\n",
    "\n",
    "result_data_frame.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_frame.to_csv('tmp/out_table.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This completes the development and deployment of the Classification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
