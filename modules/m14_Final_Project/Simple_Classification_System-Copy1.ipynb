{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP Class\n",
    "\n",
    "from pipeUtils import Document\n",
    "from pipeUtils import Annotation\n",
    "import re\n",
    "\n",
    "class PadClassificationSystem:\n",
    "    def __init__(self):\n",
    "        #initiate necessary components        \n",
    "        self.target_rules=self.getTargetRegexes()        \n",
    "        self.negation_rules = self.getNegRegexes()\n",
    "                \n",
    "    def process(self, document):\n",
    "        document_id = document.document_id\n",
    "        ann_index=0\n",
    "        for reg in self.target_rules:\n",
    "            for match in reg.finditer(document.text):\n",
    "                ann_id = 'NLP_'+ str(document_id) + '_' + str(ann_index)\n",
    "                ann_index=ann_index+1\n",
    "                new_annotation = Annotation(start_index=int(match.start()), \n",
    "                                    end_index=int(match.end()), \n",
    "                                    type='pad_annotation',\n",
    "                                    ann_id = ann_id\n",
    "                                    )\n",
    "                new_annotation.spanned_text = document.text[new_annotation.start_index:new_annotation.end_index]\n",
    "\n",
    "                # Check negation right before the found target up to 30 charachers before, \n",
    "                # making sure that the pre-text does not cross the text boundary and is valid\n",
    "\n",
    "                if new_annotation.start_index - 30 > 0:\n",
    "                    pre_text_start = new_annotation.start_index - 30\n",
    "                else:\n",
    "                    pre_text_start = 0\n",
    "\n",
    "                # ending index of the pre_text is the beginning of the found target    \n",
    "                pre_text_end = new_annotation.start_index    \n",
    "\n",
    "                # substring the document text to identify the pre_text string\n",
    "                pre_text = doc.text[pre_text_start: pre_text_end]\n",
    "\n",
    "                # We do not need to know the exact location of the negation keyword, so re.search is acceptable\n",
    "                for neg_regex in self.negation_rules:\n",
    "                    if re.search(neg_regex, pre_text):\n",
    "                        new_annotation.attributes[\"Negation\"] =\"Negated\"\n",
    "\n",
    "                document.annotations.append(new_annotation)\n",
    "        \n",
    "        return document \n",
    "    \n",
    "    def getTargetRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'(peripheral\\s*(arter\\w*|vasc\\w*)\\s*disease)',\n",
    "            r'\\bpvd\\b',\n",
    "            r'femoral.{1,50}occlusion'\n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes\n",
    "\n",
    "    def getNegRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'\\bno\\b',\n",
    "            r'no\\s*evidence\\s*of'  ,\n",
    "            r'does\\s*not\\s*have',\n",
    "            r'denies'\n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc1\r\n",
      "-------\r\n",
      "\n",
      "Patient has peripheral artery disease. ---------- \n",
      "Patient also has PVD or peripheral vascular\n",
      "disease or pvd . \n",
      "\n",
      " The patient does not have any peripheral artery disease \n",
      "but has peripheral arterial disease . The patient denies having peripheral vascular disease . \n",
      " \n",
      "The patient has a femoral and illiac occlusion which is suggestive of peripheral arterial disease.\n",
      "\r\n",
      "-------\r\n",
      "NLP_Doc1_0 pad_annotation 13 38 peripheral artery disease \r\n",
      "NLP_Doc1_1 pad_annotation 76 103 peripheral vascular\n",
      "disease \r\n",
      "NLP_Doc1_2 pad_annotation 146 171 peripheral artery disease [Negation:Negated]\r\n",
      "NLP_Doc1_3 pad_annotation 181 208 peripheral arterial disease \r\n",
      "NLP_Doc1_4 pad_annotation 237 264 peripheral vascular disease [Negation:Negated]\r\n",
      "NLP_Doc1_5 pad_annotation 340 367 peripheral arterial disease \r\n",
      "NLP_Doc1_6 pad_annotation 69 72 PVD \r\n",
      "NLP_Doc1_7 pad_annotation 107 110 pvd \r\n",
      "NLP_Doc1_8 pad_annotation 288 316 femoral and illiac occlusion \r\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  test case\n",
    "nlp_system = PadClassificationSystem()\n",
    "doc_text = '''\n",
    "Patient has peripheral artery disease. ---------- \\nPatient also has PVD or peripheral vascular\\ndisease or pvd . \n",
    "\\n The patient does not have any peripheral artery disease \n",
    "but has peripheral arterial disease . The patient denies having peripheral vascular disease . \\n \n",
    "The patient has a femoral and illiac occlusion which is suggestive of peripheral arterial disease.\n",
    "'''\n",
    "doc=Document(text=doc_text, document_id='Doc1')\n",
    " \n",
    "out_doc=nlp_system.process(doc)\n",
    "print(out_doc.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeUtils import Annotation\n",
    "from pipeUtils import Document\n",
    " \n",
    "import os\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all test documents\n",
    "unid=\"u6008208\"\n",
    "project_1 = \"Project_abi_v1\"\n",
    "project_2 = \"PAD_ann_test\"\n",
    "path_1 = \"/home/\"+\"u1166466\"+\"/BRAT/\"+str(unid)+\"/\"+project_1\n",
    "path_2 = \"/home/\"+str(unid)+\"/BRAT/\"+str(unid)+\"/\"+project_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_0.txt': <pipeUtils.Document at 0x7fe7c85df7f0>,\n",
       " '_1.txt': <pipeUtils.Document at 0x7fe7c8577668>,\n",
       " '_10.txt': <pipeUtils.Document at 0x7fe7c8572630>,\n",
       " '_11.txt': <pipeUtils.Document at 0x7fe7c8572828>,\n",
       " '_12.txt': <pipeUtils.Document at 0x7fe7c857f828>,\n",
       " '_13.txt': <pipeUtils.Document at 0x7fe7c8577e48>,\n",
       " '_14.txt': <pipeUtils.Document at 0x7fe7c85b8e80>,\n",
       " '_15.txt': <pipeUtils.Document at 0x7fe7c857b358>,\n",
       " '_16.txt': <pipeUtils.Document at 0x7fe7c85e5198>,\n",
       " '_17.txt': <pipeUtils.Document at 0x7fe7c85c9898>,\n",
       " '_18.txt': <pipeUtils.Document at 0x7fe7c85e54a8>,\n",
       " '_19.txt': <pipeUtils.Document at 0x7fe7c85c9780>,\n",
       " '_2.txt': <pipeUtils.Document at 0x7fe7c85df828>,\n",
       " '_20.txt': <pipeUtils.Document at 0x7fe7c85d9a58>,\n",
       " '_21.txt': <pipeUtils.Document at 0x7fe7c856e828>,\n",
       " '_22.txt': <pipeUtils.Document at 0x7fe7c857b8d0>,\n",
       " '_23.txt': <pipeUtils.Document at 0x7fe7c856ecf8>,\n",
       " '_24.txt': <pipeUtils.Document at 0x7fe7c8572ef0>,\n",
       " '_25.txt': <pipeUtils.Document at 0x7fe7c85b8c18>,\n",
       " '_26.txt': <pipeUtils.Document at 0x7fe7c857b048>,\n",
       " '_27.txt': <pipeUtils.Document at 0x7fe7c856a6d8>,\n",
       " '_28.txt': <pipeUtils.Document at 0x7fe7c856ab38>,\n",
       " '_29.txt': <pipeUtils.Document at 0x7fe7c857f438>,\n",
       " '_3.txt': <pipeUtils.Document at 0x7fe7c85e50b8>,\n",
       " '_30.txt': <pipeUtils.Document at 0x7fe7c856e1d0>,\n",
       " '_31.txt': <pipeUtils.Document at 0x7fe7c8572668>,\n",
       " '_32.txt': <pipeUtils.Document at 0x7fe7c8585160>,\n",
       " '_33.txt': <pipeUtils.Document at 0x7fe7c85d9438>,\n",
       " '_34.txt': <pipeUtils.Document at 0x7fe7c857b198>,\n",
       " '_35.txt': <pipeUtils.Document at 0x7fe7c857bf60>,\n",
       " '_36.txt': <pipeUtils.Document at 0x7fe7c85b8710>,\n",
       " '_37.txt': <pipeUtils.Document at 0x7fe7c85e53c8>,\n",
       " '_38.txt': <pipeUtils.Document at 0x7fe7c85d9c88>,\n",
       " '_39.txt': <pipeUtils.Document at 0x7fe7c8572da0>,\n",
       " '_4.txt': <pipeUtils.Document at 0x7fe7c85dffd0>,\n",
       " '_40.txt': <pipeUtils.Document at 0x7fe7c85c9ba8>,\n",
       " '_41.txt': <pipeUtils.Document at 0x7fe7c85df278>,\n",
       " '_42.txt': <pipeUtils.Document at 0x7fe7c8577320>,\n",
       " '_43.txt': <pipeUtils.Document at 0x7fe7c856a518>,\n",
       " '_44.txt': <pipeUtils.Document at 0x7fe7c85dfd68>,\n",
       " '_45.txt': <pipeUtils.Document at 0x7fe7c8577b38>,\n",
       " '_46.txt': <pipeUtils.Document at 0x7fe7c85d97f0>,\n",
       " '_47.txt': <pipeUtils.Document at 0x7fe7c85b8b38>,\n",
       " '_48.txt': <pipeUtils.Document at 0x7fe7c85e5588>,\n",
       " '_49.txt': <pipeUtils.Document at 0x7fe7c856a898>,\n",
       " '_5.txt': <pipeUtils.Document at 0x7fe7c856e748>,\n",
       " '_50.txt': <pipeUtils.Document at 0x7fe7c856e358>,\n",
       " '_51.txt': <pipeUtils.Document at 0x7fe7c85c9320>,\n",
       " '_52.txt': <pipeUtils.Document at 0x7fe7c8577f28>,\n",
       " '_53.txt': <pipeUtils.Document at 0x7fe7c857fa90>,\n",
       " '_54.txt': <pipeUtils.Document at 0x7fe7c85b8630>,\n",
       " '_55.txt': <pipeUtils.Document at 0x7fe7c856e940>,\n",
       " '_56.txt': <pipeUtils.Document at 0x7fe7c85e5940>,\n",
       " '_57.txt': <pipeUtils.Document at 0x7fe7c857f240>,\n",
       " '_58.txt': <pipeUtils.Document at 0x7fe7c85c9588>,\n",
       " '_59.txt': <pipeUtils.Document at 0x7fe7c85d99b0>,\n",
       " '_6.txt': <pipeUtils.Document at 0x7fe7c8572160>,\n",
       " '_60.txt': <pipeUtils.Document at 0x7fe7c856a198>,\n",
       " '_61.txt': <pipeUtils.Document at 0x7fe7c85c9710>,\n",
       " '_62.txt': <pipeUtils.Document at 0x7fe7c85b8828>,\n",
       " '_63.txt': <pipeUtils.Document at 0x7fe7c85dfba8>,\n",
       " '_64.txt': <pipeUtils.Document at 0x7fe7c85e5dd8>,\n",
       " '_65.txt': <pipeUtils.Document at 0x7fe7c85777b8>,\n",
       " '_66.txt': <pipeUtils.Document at 0x7fe7c857b5c0>,\n",
       " '_67.txt': <pipeUtils.Document at 0x7fe7c856ee48>,\n",
       " '_68.txt': <pipeUtils.Document at 0x7fe7c856a320>,\n",
       " '_7.txt': <pipeUtils.Document at 0x7fe7c85e5e10>,\n",
       " '_8.txt': <pipeUtils.Document at 0x7fe7c857fd68>,\n",
       " '_9.txt': <pipeUtils.Document at 0x7fe7c8572128>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs=dict()\n",
    "test_doc_paths = glob.glob(str(path_1+'/*.txt')) \n",
    "for d in test_doc_paths:\n",
    "    doc = Document()\n",
    "    #print(d)\n",
    "    doc.load_document_from_file(d)\n",
    "    #print(str(d[:-3])+'ann')\n",
    "    doc.load_annotations_from_brat(str(d[:-3])+'ann')\n",
    "    #print(os.path.basename(d))\n",
    "    test_docs[os.path.basename(d)]=doc\n",
    "\n",
    "\n",
    "test_docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the all notes\n",
    "nlp_system = PadClassificationSystem()\n",
    "\n",
    "for doc_id in  test_docs.keys():\n",
    "    nlp_system.process(test_docs.get(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 0 FP = 38 FN = 0\n",
      "NLP__51.txt_0 pad_annotation 424 427 pvd \n",
      "NLP__38.txt_0 pad_annotation 414 417 PVD \n",
      "NLP__54.txt_0 pad_annotation 426 429 pvd \n",
      "NLP__25.txt_0 pad_annotation 457 460 PVD \n",
      "NLP__36.txt_0 pad_annotation 431 434 PVD \n",
      "NLP__2.txt_0 pad_annotation 19647 19674 Peripheral Vascular Disease \n",
      "NLP__2.txt_1 pad_annotation 427 430 pvd \n",
      "NLP__2.txt_2 pad_annotation 1919 1922 PVD \n",
      "NLP__2.txt_3 pad_annotation 3980 3983 PVD \n",
      "NLP__2.txt_4 pad_annotation 17927 17930 PVD \n",
      "NLP__44.txt_0 pad_annotation 455 458 pvd \n",
      "NLP__4.txt_0 pad_annotation 241 268 PERIPHERAL VASCULAR DISEASE \n",
      "NLP__49.txt_0 pad_annotation 215 218 PVD \n",
      "NLP__49.txt_1 pad_annotation 545 548 PVD \n",
      "NLP__30.txt_0 pad_annotation 269 272 PVD \n",
      "NLP__30.txt_1 pad_annotation 467 470 PVD \n",
      "NLP__5.txt_0 pad_annotation 761 788 peripheral vascular disease \n",
      "NLP__5.txt_1 pad_annotation 357 360 PVD \n",
      "NLP__55.txt_0 pad_annotation 251 278 PERIPHERAL VASCULAR DISEASE \n",
      "NLP__55.txt_1 pad_annotation 822 849 peripheral vascular disease \n",
      "NLP__55.txt_2 pad_annotation 453 456 PVD \n",
      "NLP__23.txt_0 pad_annotation 207 210 PVD \n",
      "NLP__23.txt_1 pad_annotation 3156 3159 PVD \n",
      "NLP__31.txt_0 pad_annotation 217 220 PVD \n",
      "NLP__31.txt_1 pad_annotation 778 781 PVD \n",
      "NLP__39.txt_0 pad_annotation 468 471 PVD \n",
      "NLP__39.txt_1 pad_annotation 1013 1016 PVD \n",
      "NLP__39.txt_2 pad_annotation 3732 3735 PVD \n",
      "NLP__1.txt_0 pad_annotation 188 191 PVD \n",
      "NLP__1.txt_1 pad_annotation 373 376 PVD \n",
      "NLP__1.txt_2 pad_annotation 460 463 PVD \n",
      "NLP__1.txt_3 pad_annotation 3088 3091 PVD \n",
      "NLP__45.txt_0 pad_annotation 419 422 pvd \n",
      "NLP__52.txt_0 pad_annotation 174 177 PVD \n",
      "NLP__52.txt_1 pad_annotation 481 484 PVD \n",
      "NLP__22.txt_0 pad_annotation 358 361 PVD \n",
      "NLP__26.txt_0 pad_annotation 558 585 peripheral vascular disease \n",
      "NLP__26.txt_1 pad_annotation 334 337 pvd \n"
     ]
    }
   ],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).compare_types_by_span('PAD','pad_annotation', False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "for a in tp_list_total:\n",
    "    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "attributes_to_compare=[]\n",
    "# To compare attributes, create a list of tuples for each pair to compare:\n",
    "# attributes_to_compare.append[(A1_type, A1_att_name, A1_att_value),(A2_type, A2_att_name, A2_att_value)]\n",
    "attributes_to_compare.append([('PAD', 'Negation', 'Negated'),('pad_annotation', 'Negation', 'Negated')])\n",
    "\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).\\\n",
    "    compare_types_by_span_and_attributes('PAD','pad_annotation', attributes_to_compare , False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "#for a in tp_list_total:\n",
    "#    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),\n",
    "                       db='mimic2')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify patients with PAD for reference standard\n",
    "pad_data = pd.read_sql(\"\"\"SELECT noteevents.subject_id, \n",
    "                      noteevents.category, \n",
    "                      noteevents.text FROM noteevents limit 100000 \"\"\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn.close()\n",
    "except:\n",
    "    print(\"Connection is already closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pad_data.columns)\n",
    "print(\"Number of records = \", len(pad_data))\n",
    "\n",
    "pad_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nlp_system = PadClassificationSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "counter = 0\n",
    "for index , row in pad_data.sample(10000).iterrows():    \n",
    "    doc = Document(document_id=str(row.subject_id) + '_' + str(index), text=row.text)\n",
    "    final_nlp_system.process(doc)\n",
    "    if(len(doc.annotations) > 0):\n",
    "        i = 1\n",
    "        for a in doc.annotations:\n",
    "            if( a.type == 'pad_annotation'):\n",
    "                neg_flag = 0\n",
    "                # Switch the flag to 1 when the mention is negated\n",
    "                if('definite_negated_existence' in a.attributes):\n",
    "                    neg_flag=1\n",
    "                ### Each row in the dictionary\n",
    "                record_id  = str(row.subject_id) + '_' + str(index)+'_'+str(i)\n",
    "                subject_id =  row.subject_id\n",
    "                note_id = str(row.subject_id) + '_' + str(index)\n",
    "                annotation_type = a.type\n",
    "                snippet = doc.text[int(a.start_index): int(a.end_index)]\n",
    "                out_list = [record_id, subject_id, note_id, annotation_type, \\\n",
    "                            a.start_index, a.end_index, \\\n",
    "                            snippet, neg_flag]\n",
    "                output.append(out_list)\n",
    "                i=i+1\n",
    "                counter=counter+1\n",
    "                # Print . after 10 identified records\n",
    "                if counter%10 == 0:\n",
    "                    print('.', end='')\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['record_id','subject_id', 'note_id', 'annotation_type', 'span_start', 'span_end', 'PAD_snippet', 'neg_flag']\n",
    "result_data_frame = (pd.DataFrame(output, columns=columns))\n",
    "\n",
    "result_data_frame.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_frame.to_csv('tmp/out_table.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This completes the development and deployment of the Classification system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
