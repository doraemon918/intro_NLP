{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to the scikit-learn machine learning environment\n",
    "\n",
    "In this module, we will work through the steps of retrieving data on biomedical literature articles in PubMed and we will introduce ourselves to the very comprehensive machine learning environment, ***scikit-learn***.\n",
    "\n",
    "Specifically we will:\n",
    "1. Create a simple function that does a lot of work: when passed a proper PubMed query, it will return a dictionary full of PMIDs and their abstracts generated by PubMed. \n",
    "2. Learn how to take information stored in dictionaries, like the abstracts above,and write them to a directory as text files on disk. These files can be used by really any program that is expecting a directory of texts (scikit-learn, BRAT, etc.)\n",
    "3. See how to do some basic NLP text preprocessing like i) filtering out stop words; ii) tokenization; iii) mapping all characters to lowercase; iv) and stemming.\n",
    "4. Finally, we will run some simple machine learning exercises on the abstracts we extracted in Part 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this cell if you already have executed it in a previous notebook\n",
    "#import nltk\n",
    "#nltk.download() #enter 'd' at 'Downloader>' prompt; then enter 'book' at 'Identifier>' prompt; enter 'q' to quiit.\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('perluniprops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez, Medline\n",
    "import nltk\n",
    "import gensim\n",
    "import operator\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Biopython package to retreive abstracts from PubMed\n",
    "<p><font color='darkblue'>Biopython is a Python package designed for computational molecular biology and bioinformatics. It mainly contains tools useful for analyzing genetic and sequence data, but it also contains a set of functions that allow us to query against PubMed easily. </font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But first: Why would we want to focus on abstracts? \n",
    "<p><font color='darkblue'>Abstracts from articles in the biomedical literature are useful for a number of purposes. One of the most important is that reviewing abstracts is usually the first step  in building a systematic review.\n",
    "\n",
    "**First, we need a function that collects records from PubMed when given a proper PubMed search query:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryAbstracts(search_terms : str, max = 5):    \n",
    "    '''This function takes a string containing the usual PubMed terms we would use with\n",
    "    PubMed itself and returns a dictionary of matching PubMed IDs (dictionary keys) and matching PubMed \n",
    "    abstracts (dictionary values) that PubMed returns when queried with search_terms, limited to \"max\" abstracts.\n",
    "    \n",
    "    It handles all the messy URL communication AND the messy XML format returned by PubMed. Credit the\n",
    "    Biopython package for making that so easy...\n",
    "    '''\n",
    "    Entrez.email = \"john.hurdle@utah.edu\" #NLM likes to know who is sending requests to its PubMed APIs\n",
    "    Entrez.tool = \"Biopython\" #recommended but not necessary\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term = search_terms, retmax = max) #these next 4 lines talk \n",
    "    record = Entrez.read(handle)                                            #to PubMed to find IDs matching search terms\n",
    "    idlist = record[\"IdList\"]\n",
    "    handle.close()\n",
    "    \n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\") #these next 10 lines talk \n",
    "    records = Medline.parse(handle)                                                   #talk to PubMed to query for the abstracts\n",
    "    abstracts=dict()  # create the empty dictionary\n",
    "    for record in records: #for each ID returned above, see if it has an abstract (e.g., letters to editor don't)\n",
    "        if 'AB' in record: #if it does, entry text of abstract under the PMID as key in dictionary\n",
    "            pmid=record['PMID']\n",
    "            abstracts[pmid]=record['AB']\n",
    "        if len(abstracts)== max: #once we hit max, we quit\n",
    "            break;\n",
    "    handle.close()\n",
    "    \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='darkblue'>Let's try to search for a set of abstracts that can be compared to each other for classification purposes. First, we will grab up to 200 abstracts in PubMed for which the National Library of Medicine librarians have indexed the articles under the MeSH headings of \"natural language processing\". **This is an example of using an external data source to add value to an ML task.**\n",
    "    \n",
    "Here we are using the skill of the NLM librariians to establish the refrence (or \"gold\") standard, in lieu of annotation in clinical notes. We focus here on abstracts, but we could, with a little more effort, use full-text articles from PubMedCentral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_NLP = queryAbstracts('\"natural language processing\"[mesh]\"',200) # You can put any query that the work in PubMed inside the single quotes hereâ€¦\n",
    "len(abstracts_NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='darkblue'>Then we will grab up to 200 abstracts in PubMed for which the National Library of Medicine \n",
    "librarians have indexed the articles under the MeSH headings of \"artificial intelligence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_AI = queryAbstracts('\"artificial intelligence\"[MeSH Terms] \\\n",
    "                               NOT \"natural language processing\"[All Fields]',200)\n",
    "len(abstracts_AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why choose \"artificial intelligence? Why do we need the NOT in the AI query?\n",
    "**The MeSH tree for \"natural language processing\":**\n",
    "\n",
    "    All MeSH Categories\n",
    "        Information Science Category\n",
    "            Information Science\n",
    "                Computing Methodologies\n",
    "                    Algorithms\n",
    "                        Artificial Intelligence\n",
    "                            Natural Language Processing <--- HERE ********************************\n",
    "\n",
    "**The MeSH tree for \"artificial intelligence\":**\n",
    "\n",
    "    All MeSH Categories\n",
    "        Information Science Category\n",
    "            Information Science\n",
    "                Computing Methodologies\n",
    "                    Algorithms\n",
    "                        Artificial Intelligence <--HERE ********************************\n",
    "                            Computer Heuristics\n",
    "                            Expert Systems\n",
    "                            Fuzzy Logic\n",
    "                            Knowledge Bases\n",
    "                                Biological Ontologies +\n",
    "                            Machine Learning\n",
    "                                Supervised Machine Learning +\n",
    "                                Unsupervised Machine Learning\n",
    "                            Natural Language Processing <--HERE ********************************\n",
    "                            Neural Networks (Computer)\n",
    "                            Robotics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='darkblue'>***If it is important to your particular task,*** the code in this next cell will ensure that both directories (files are written to disk further down in the code) are of equal size, that is, that they are equally balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_abs_remove = abs(len(abstracts_NLP) - len(abstracts_AI)) #see how many files need to be trimmed from larger set\n",
    "if num_abs_remove == 0:\n",
    "    pass #the classes are balanced, full speed ahead\n",
    "elif len(abstracts_NLP) > len(abstracts_AI): #then remove the extra positive files\n",
    "    abs_count = 0\n",
    "    del_list = []\n",
    "    for del_key in abstracts_NLP.keys():\n",
    "        del_list.append(del_key)\n",
    "        abs_count += 1\n",
    "        if abs_count >= num_abs_remove: \n",
    "            for del_key in del_list:\n",
    "                abstracts_NLP.pop(del_key, None)\n",
    "            break\n",
    "else:  #then (abstracts_AI) > len(abstract_NLP): so remove the extra negative files\n",
    "    abs_count = 0\n",
    "    del_list =[]\n",
    "    for del_key in abstracts_AI.keys():\n",
    "        del_list.append(del_key)\n",
    "        abs_count += 1\n",
    "        if abs_count >= num_absL_remove: \n",
    "            for del_key in del_list:\n",
    "                abstracts_AI.pop(delkey, None)\n",
    "            break\n",
    "print(\"num NLP: \", len(abstracts_NLP), \" num AI: \", len(abstracts_AI))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the dataset\n",
    "\n",
    "#### <p><font color='darkblue'>Some common preprocessing tax for clinical text are::<br></font></p>\n",
    "* **shuffle the documents:** so that repeated runs use a new ordering (keeps us honest...) **Doh, scikit can do this, too**\n",
    "* **lowercase words:** so words like \"Library\" and \"library\" look the same to the ML ***(any problems with this idea?)*** **Doh, scikit can do this, too**\n",
    "* **stem the words:** so words like \"blood\", \"bled\", bleeding\" look the same to the ML\n",
    "* **remove stop words:** so common words like \"a\", \"the\", etc. are not considered by the ML **Doh, scikit can do this, too**\n",
    "* **remove digits:** so numbers don't inlfuence the ML\n",
    "\n",
    "It would be enormously useful to have a pre-processors tool that ** expands acronyms and short forms**, But that is harder than it sounds to do really well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code sets up a string of punctuation characters and a useful list of stopwords.\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We won't run this now since scikit can tokenize and stem the input text if we want. But it is better to make pre-processing options obvious when we run the data fetch, but I include the code for your reference.\n",
    "\n",
    "Tokenize and stem all words in positive abstracts, then put it back into the abstract text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and stem all words in positive abstracts, then put it back into the absrtact text field\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer = MosesTokenizer()\n",
    "\n",
    "for abstract_id in doc_ids_NLP:  #loop through all of the shuffled positive abstracts\n",
    "    processed_abstract = '' #start with a blank string for each abstract\n",
    "    for word in tokenizer.tokenize(abstracts_NLP[abstract_id]): #tokenize and stem each word.\n",
    "        if not word.replace('-','',1).isdigit() \\\n",
    "        and not word in my_stopwords: \\\n",
    "            processed_abstract += ' ' +stemmer.stem(word.lower()) #concatenate stemmed word into new abstract\n",
    "    abstracts_NLP[abstract_id] = processed_abstract  #replace non-processed text with processed text\n",
    "##!! Need to write full (non-stemmed, non lower case)text files automatically under 'abstracts_full_<pos | neg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ditto, not run here...\n",
    "Tokenize and stem all words in negative abstracts, then put it back into the abstract text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and stem all words in negative abstracts, then put it back into the abrstact text field\n",
    "for abstract_id in doc_ids_AI:  #loop through all of the abstracts\n",
    "    processed_abstract = '' #start with a blank string for each abstract\n",
    "    for word in tokenizer.tokenize(abstracts_AI[abstract_id]): #tokenize and stem each word.\n",
    "        if not word.replace('-','',1).isdigit() \\\n",
    "        and not word in my_stopwords: \\\n",
    "            processed_abstract += ' ' +stemmer.stem(word.lower()) #concatenate stemmed word into new abstract\n",
    "    abstracts_AI[abstract_id] = processed_abstract  #replace non-processed text with processed text\n",
    "##!! Need to write full (non-stemmed, non lower case) text files automatically under 'abstracts_full_<pos | neg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now write (or re-write) the pre-processed files to directories so we can import them into scikit-learn or into BRAT or wherever we want...\n",
    "<p><font color='darkblue'>Note that there is a hardcoded flag, called dir_overwrite, that prevents files from being accidentally overwritten. If this entire notebook were wrapped up into a program, then that flag would likely be set by a function call for input statement to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil as sh\n",
    "\n",
    "###############################\n",
    "dir_overwrite = False  # set to True if you REALLY want to overwrite your previous files\n",
    "###############################\n",
    "\n",
    "my_dir =\"./data/\"  \n",
    "if not os.path.exists(my_dir): #check to see inthe data directory exists; if not, make it\n",
    "    os.mkdir(my_dir)\n",
    "path_NLP = my_dir+\"abstracts_NLP/\"\n",
    "path_AI = my_dir+\"abstracts_AI/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_NLP) & dir_overwrite:  # then remove old dir the positive abstracts file\n",
    "    sh.rmtree(path_NLP)  #remove the old dir and its files\n",
    "    print(\"re-writing positive files\\n\")\n",
    "if os.path.exists (path_AI) & dir_overwrite:  # then overwirte the positive abstracts file\n",
    "    sh.rmtree(path_AI)  #remove the old dir and its files\n",
    "    print(\"re-writing negative files\\n\") \n",
    "if not os.path.exists(path_NLP):\n",
    "    os.mkdir(path_NLP)#make new empty dir\n",
    "if not os.path.exists(path_AI):\n",
    "    os.mkdir(path_AI)#make new empty dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dir_overwrite:  # only overwite bot sets of files if overwrite is True\n",
    "    for key in abstracts_NLP.keys(): #loop trhough each positve abstract and build a text file\n",
    "        text = abstracts_NLP[key]  #get the text for this pubmed id\n",
    "        myfile = open(path_NLP+key+\".txt\",'w') #write the text under a file name of the pubmed id\n",
    "        myfile.write(text)\n",
    "        myfile.close()\n",
    "    print (\"--> Finished writing positives...\\n\")\n",
    "    \n",
    "    for key in abstracts_AI.keys(): #loop trhough each negative abstract and build a text file\n",
    "        text = abstracts_AI[key]  #get the text for this pubmed id\n",
    "        myfile = open(path_AI+key+\".txt\",'w') #write the text under a file name of the pubmed id\n",
    "        myfile.write(text)\n",
    "        myfile.close()\n",
    "    print (\"--> Finished writing negatives...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "### Okay, we are ready to play with sci-kit-learn!\n",
    "### ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "### First:\n",
    "<p><font color='darkblue'>We load the files into scikit-learn. The scikit designers did something very clever here: they just require you organize the target texts into separate directories by class type. So here all of the positive abstracts (i.e., NLP articles) are in one directory and all the negative (i.e., AI but not NLP) articles are in another. If you have three classes to classify, you'd have three directories. The name of the sub-directory becomes the class name in the classifer model. Neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "my_dir = \"./data/\" \n",
    "my_bunch = sklearn.datasets.load_files(my_dir, #scikit uses subdir names as class names \\\n",
    "                            description = \"This dir holds two sets of abstracts. The abstract_<pos | neg> holds ~200 files each of text of PubMed abstracts that are positive or negative for some category, where each token in files is stemmed and lowercase. The abstract_full_<pos | neg> holds same text files with no stemming or lower case.\", \\\n",
    "                            categories = None, #just process the subdirs as classes \\\n",
    "                            load_content = True, #loads actual text into scikit -- not sure what this means if False \\\n",
    "                            shuffle = True,      #mixes up order to keep the ML honest \\\n",
    "                            encoding = 'utf-8',  #required flag for text files but could be none for image files or other non-text) \\\n",
    "                            decode_error = 'strict',#optional \\\n",
    "                            random_state = 0) # can change to another integer to get different start state \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --as a sanity check, let's print some stats on the \"bunch\", the scikit-learn basic data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(my_bunch.target)\n",
    "print(my_bunch.target_names)\n",
    "print(\"sample target values: \",my_bunch.target[0:10])\n",
    "for t in my_bunch.target[:10]:\n",
    "    print(my_bunch.target_names[t], \" | \", end = \"\")\n",
    "print(\"\\n\",len(my_bunch.filenames)) #count number of file names\n",
    "print(len(my_bunch.data), \"-- should be same as above\")  # count number of files' data sets -- should be same as above\n",
    "print(\"\\n\".join(my_bunch.data[0].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second:\n",
    "<p><font color='darkblue'>We specifiy a tf-idf vector maker as our first pass at features for our machine learning task. We can do a lot of **pre-processing** here using the options below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can safely ignore any parameter here that have no comments\n",
    "# Details are here: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#First, make a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer(#input = my_bunch, # From the load files above\\\n",
    "                        encoding = 'utf-8', \\\n",
    "                        decode_error = 'strict', \\\n",
    "                        strip_accents = None, \\\n",
    "                        lowercase = True, \\\n",
    "                        preprocessor = None, #could put our stemming function here \\\n",
    "                        tokenizer = None, #could put our stemming or tokenizer here \\\n",
    "                        analyzer = 'word', # could be 'char' for character models \\\n",
    "                        stop_words = 'english', #use None if no stop words desired \\\n",
    "                        token_pattern = '(?u)\\b\\w\\w+\\b', \\\n",
    "                        ngram_range = (1, 2), #use 1's here for unigrams, 2's for bigrams... \\\n",
    "                        max_df = 0.1, min_df = 1, \\\n",
    "                        max_features = None, #can limit the length of feature vectors here, otherwise use them all\\\n",
    "                        vocabulary = None, \\\n",
    "                        binary = False, \\\n",
    "                        dtype = 'numpy.int64', \\\n",
    "                        norm = 'l2', #L2 is standard \"norming function\" to use here\\\n",
    "                        use_idf = True, \\\n",
    "                        smooth_idf =  True, \\\n",
    "                        sublinear_tf= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third:\n",
    "<p><font color='darkblue'>We use scikit to count each token across the corpus of positive and negative abstracts. **Bag of Words!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#make a vectorizer instance\n",
    "count_vect = CountVectorizer() \n",
    "#apply it to our data\n",
    "my_train_counts = count_vect.fit_transform (my_bunch.data)\n",
    "my_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth:\n",
    "<p><font color='darkblue'>We use the scikit tf-idf vectorizer that we defined above to convert the counts into tf-idf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#make a vectorizer instance \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "#apply it to our data\n",
    "my_train_tfidf = tfidf_transformer.fit_transform(my_train_counts)\n",
    "my_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is time to choose a machine learning model. \n",
    "<p><font color='darkblue'>So let's start with the universally recommended Naive-Bayes model for the baseline. Scikit makes changing models really simple: just change these two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(my_train_tfidf,my_bunch.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='darkblue'>At this point we could build a proper test corpus, but it would be hard to see in a cell why a given abstract would be classified one way or the other. So we'll just make up a few toy abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abstracts = ['This article uses natural language processing to push science forward', \\\n",
    "   \n",
    "                  'Robots push society backwards', \\\n",
    "                  \n",
    "                  \"Roses are red, violets are snappy, if NLP was easy I'd be happy!\",\\\n",
    "                 \n",
    "                  'Micro blots are fun', \\\n",
    "                 \n",
    "                  '[Actual AI/non-NLP abstract not seen yet]Details are given of the application of Artificial Neural Networks (ANNs) to predicting the compliance of bathing waters along the coastline of the Firth of Clyde, situated in the south west of Scotland, UK. Water quality data collected at 7 locations during 1990-2000 were used to set up the neural networks. In this study faecal coliforms were used as a water quality indicator, i.e. output, and rainfall, river discharge, sunlight and tidal condition were used as input of these networks. In general, river discharge and tidal ranges were found to be the most important parameters that affect the coliform concentration levels. For compliance points close to the meteorological station, the influence of rainfall was found to be relatively significant to the concentration levels.' \\\n",
    "                 ]\n",
    "X_new_counts = count_vect.transform(test_abstracts)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(test_abstracts, predicted):\n",
    "    print('\\n%r => %s' % (doc, my_bunch.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, for somethiing toally cool: let's run  some scikit code that will take the abstract files from above, read them in, and run a pipeline that ***repetitively*** executes the Naive Bayes model while trying different parameters for that model, to see which parameter combination works the best for this data set. This is called \"parameter search\". You can substitute other scikit learning models in line \"`('clf', MultinomialNB())`\" BUT you uneed to find the parameters for the model in the scikit documentation and specificy them in the \"`parameters = {...`\" dictionary code below. <p><font color='darkblue'>NB: this takes about a minute to run and generates a lot of warnings...scroll down to the resuts at the bottom.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "#print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = None\n",
    "\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading abstracts...\") #print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "#print(categories)\n",
    "my_dir = \"./data/\" \n",
    "data = sklearn.datasets.load_files(my_dir, #scikit uses subdir names as class names \\\n",
    "                                   description = \"This dir holds two sets of abstracts. The abstract_<pos | neg> holds ~200 files each of text of PubMed abstracts that are positive or negative for some category, where each token in files is stemmed and lowercase. The abstract_full_<pos | neg> holds same text files with no stemming or lower case.\", \\\n",
    "                                   categories = None, #just process the subdirs as classes \\\n",
    "                                   load_content = True, #loads actual text into scikit -- not sure what this means if False \\\n",
    "                                   shuffle = True,      #mixes up order to keep the ML honest \\\n",
    "                                   encoding = 'utf-8',  #required flag for text files but could be none for image files or other non-text) \\\n",
    "                                   decode_error = 'strict',#optional \\\n",
    "                                   random_state = 0)\n",
    "#data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.05, 0.1, 0.3, 0.4,0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1,3),(1,4), (1,5), (1,6)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #use for SGDClassifier()...'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
