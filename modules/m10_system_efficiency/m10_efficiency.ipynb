{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Efficiency...the clock matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In Module 7 we discussed NLP system performance in terms of the *effectiveness* of an NLP system compared to human annotations. We focused on measures like recall, precision, and accuracy. In this notebook we will explore another measure of NLP systems, system *efficiency*. Testing an NLP system's efficiency asks, \"How fast is it?\" Or, put another, more practical way, \"Is the system *fast enough?*\"\n",
    "\n",
    "In the real world of clinical NLP, increasingly we want to process ever more notes. NLP technology, and the hardware that supports it, improved dramatically over the past 15 years. Critically, hardware costs have plummeted at the same time as the HITECH Act made vast quantities of clinical data easier to create and exchange.\n",
    "\n",
    "But as we process more and more notes we need to be mindful of how long it takes to do so. Processing efficiency becomes increasingly important as the size of clinical datasets grows. In clinical natural language processing (NLP), even small increases in processing throughput are important when handling very large note corpora. \n",
    "### The Clock on the Wall\n",
    "An extreme example is the U.S. Veterans Administration VINCI national data warehouse, which contains 2-3 billion notes. Divita et al. demonstrated the effect of note processing efficiency in VINCI studies (see the required article for this Module). In their work, 6 million records is considered a representative national sample for many applications. Shaving off 100 milliseconds (i.e., one-tenth of second) of processing time per note in their benchmark system (401 milliseconds was their nominal per-note-processing time) **would save nearly a week** of clock time for a corpus of that size.\n",
    "\n",
    "To assess efficiency, we focus on “clock time,” the amount of real time, from the human perspective, a process takes to complete. To put clock time into a *real-world context, see the real-time, near real-time, and daily examples* in the Module 10 lecture. In this notebook, let’s explore the scenario (a very practical one as you start to build your project!) where you, as a researcher, are tweaking an NLP algorithm and have to wait for processing after each tweak.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `timeit` library\n",
    "First we need some Python tools that can provide a stopwatch for measuring how long Python processes take to run. These will allow us to see where clock-time bottlenecks are. For more information on the timeit library, [look here in the Python documention.](https://docs.python.org/2/library/timeit.html#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, let's import what we need...\n",
    "import timeit\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a test function that we can use to practice:\n",
    "def test():\n",
    "    '''A test function that builds a list of the first 10,000 integers by appending them -- nothing fancy'''\n",
    "    mylist = []\n",
    "    for i in range(100000):\n",
    "        mylist.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Timeit` provides a new class and a few methods to measure the time it takes to execute statements. To keep things simple, we won't worry about the class business now, just focus on the methods. One useful method is: \n",
    "\n",
    "`timeit.repeat(stmt = '<the Python code statement you intend to time>', \\\n",
    "    setup= '<any Python code to run each time the REPEAT below starts>' \\  #optional\n",
    "    repeat = <number>, \\ #i.e., how many times to REPEAT the number of runs below AND average the results \\\n",
    "    number = <number>) #how many times to run the stmt code, i.e.,the TOTAL runs is (repeat*number)`\n",
    "\n",
    "In the first example, we will run test(), which appends 10,000 times, for 10 batches (i.e., `number = 10`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Run test() in loops of 10, average those, and repeat 3 times (the default if repeat is not specified):\n",
    "runtime = timeit.repeat(stmt = 'test()', \n",
    "                        setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                        number = 10)\n",
    "print(' The default repeat is 3, so 30 runs of test(), or **300,000** appends total\\n for a series of ', \\\n",
    "      runtime, ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pretty fast. \n",
    "\n",
    "How may 100ths of seconds, on average, did the three runs take to append 10,000 times in`test()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(runtime),\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Now run test() in batches of 100 and repeat 10 times, then average the time for each repeat loop\n",
    "runtime = (timeit.repeat(stmt = 'test()', \n",
    "                         setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                         repeat = 10, \n",
    "                         number = 100))\n",
    "print('Repeat 10 batches of 100 runs and average the times for batch, **10,000,000** appends total\\n for a series of ', runtime, ' seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(runtime),\"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pretty fast. Compare the overall average for both runs above...is it what you expected? \n",
    "\n",
    "Run the two cells above again. You will see that the mean time is slightly different. **Why is that?** Because the CPU is multitasking. If this Python notebook was the only process running on the CPU, the times would be much more consistent. But you are sharing the CPU with the rest of the class and with everyone else using the server. And, of course, the server has several tasks of its own to do. **That is why, when timing code, it's important to average over several runs to smooth out the interference from other processes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Okay, now let's increase the number of runs one more time: Run test() in batches of 1,000, average \\\n",
    "# the results, and repeat 5 times:\n",
    "print(\"--> WAIT for it!\")\n",
    "runtime = timeit.repeat(stmt = 'test()', \n",
    "                    setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                    repeat = 3, \n",
    "                    number = 1000)\n",
    "print(\"\\nThe average time to append 10,000 integers 1,000 times (from 3 repeats) is \", np.mean(runtime), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, do these execution times for the three examples seem about what you expected? **It is reassuringly linear**, that is, every time we increase the number of runs by a factor of 10 the execution time also increases roughly by a factor of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets time some NLP processes...\n",
    "One annoying feature of `timeit` is that it really only wants to clock a single statement. That means that you need to put the code that you want to time inside of a function. It also means you need to `import` that function's parameters, as shown below. *If you find a way to time multiple statements in timeit, do let the class know! The documentation implies it is possible, but it is not clear to us how....* \n",
    "\n",
    "We are going to pinch code from the implementation notenook back in Module 7, the code that segments a note and then runs the PyConText assertion process on those segments. **Note:** in your project, and in the real world, you would be processing hundreds or thousands of notes in a typical pipeline run. We *simulate* that in this exercise by timing the pipeline as it runs on one note hundred of times, over and over. This is not a brilliant approach to simulation, because any given note might be particularly easy or particularly hard for segmentation and context assertion. \n",
    "\n",
    "**At the end of the notebook, if you are you feeling adventurous, pinch the code from an earlier notebook, read in a hundred notes, and loop over them with the timer. :) If you do that, you will notice a decidedly different mean runtime per note.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the usual imports\n",
    "from pipeUtils import Document\n",
    "from pipeUtils import Annotation\n",
    "\n",
    "from wrapperPyConText import ConTextPipe\n",
    "from PyRuSH.RuSH import RuSH\n",
    "\n",
    "#Now create a document instance and populate it with grab a single note and its annotations\n",
    "doc = Document() #create a document instance\n",
    "doc.load_document_from_file('data/test.txt') #use the document methods to read in the note and its annotations\n",
    "doc.load_annotations_from_brat('data/test.ann')\n",
    "\n",
    "#Next, read in the sementer rule set, in the KB directory, and create a sentence segmenter instance\n",
    "sentence_rules= 'KB/rush_rules.tsv'\n",
    "sentence_segmenter = RuSH(sentence_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information extraction rule set below is teeny. This is really a toy set of rules; a real pipeline will have far more rules and thus will run more slowly than our experiments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the information extraction rules. This is a toy set, a real pipeline will have many ruch rules.\n",
    "#and thus a real pipeline will be slower.\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: pain\n",
    "Regex: ''\n",
    "Type: Depression_Symptom\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: depression\n",
    "Regex: 'depres\\\\w+'\n",
    "Type: Depression_Symptom\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: suicidal\n",
    "Regex: 'suicidal\\\\s*ideation'\n",
    "Type: Depression_Symptom\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `timeit`, we need to wrap the segmenter and context code into a function we can time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_anns(document):\n",
    "    '''A test function that wrap a document segmenter and context asserter so we can time the runtime of both.'''\n",
    "    #segment the document's sentences\n",
    "    sentences=sentence_segmenter.segToSentenceSpans(doc.text)\n",
    "    i=0\n",
    "    for sentence in sentences: \n",
    "        i=i+1  #incrementing the index\n",
    "        doc.annotations.append(Annotation(start_index=sentence.begin,\n",
    "                                          end_index=sentence.end, \n",
    "                                          type=\"Sentence\", \n",
    "                                          ann_id='Sent_'+str(i)))\n",
    "    #now run context on each sentence segment\n",
    "    context_pipe = ConTextPipe(target_rules, rule_set)\n",
    "    doc.annotations = [] #a subtle 'persistence' side-effect here; if not reset it accumulates across runs\n",
    "    for a in doc.annotations: #loop through the sentences and run the context algorithm over each\n",
    "        if a.type == \"Sentence\":\n",
    "            b = context_pipe.process(a, doc.text)\n",
    "            doc.annotations.extend(b)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set to time the new \"segmenter-context\" function. We will make three runs, each time adding more context rules. More rules mean more *effective* context assertion, but that comes at the price of lower efficiencey (slower performance), as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_sets = ['KB/general_modifiers_50-rules.yml', \\\n",
    "             'KB/general_modifiers_half-rules.yml', \\\n",
    "             'KB/general_modifiers_all-rules.yml']\n",
    "\n",
    "for rule_set in rule_sets:\n",
    "    runtime = timeit.repeat('do_anns(doc)',\n",
    "                            setup = 'print(\".\", end = \"\"); from __main__ import do_anns, doc', #ugh...note the import\n",
    "                            repeat = 3, \n",
    "                            number = 10) ##simulates a 10 note run\n",
    "    print(\"Time(s) for rule set \",rule_set,\"for 10 notes: \", runtime)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The \"half-rules.yml\" context rule set has about 190 rules, while the \"all-rules.yml\" set has about 380 rules. Yet these runs times are **not so linear**. Why is that, do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Try reading a hundred notes from MIMIC II and timing the pipeline here on each note, rather than simply procesing the same note repeatedly like we do above. To ge you started, here's the SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_sql(\"SELECT DISTINCT * FROM noteevents limit 100\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE YOUR timer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's ours:\n",
    "print(notes.text[0:6])\n",
    "rule_set = 'KB/general_modifiers_all-rules.yml'\n",
    "for index, note in notes.iterrows():\n",
    "    new_note = note.text.replace('\\n',' ')\n",
    "    doc = Document()\n",
    "    doc.text = new_note  #sets up the text for the next note to run throgh the pipe\n",
    "    doc.load_annotations_from_brat('data/test.ann')#since we don't have annotations yet, just use the test annotations\n",
    "    if re.match(r'(\\s*)(\\w+)', doc.text) != None:   #make sure the note has characters in it, else RUSH could crash\n",
    "        runtime = timeit.repeat('do_anns(doc)',\n",
    "                                setup = 'from __main__ import do_anns, doc', #ugh...note the import\n",
    "                                repeat = 1, \n",
    "                                number = 1)\n",
    "        print(\"Time(s) for note \",index,\": \", runtime)\n",
    "    else:\n",
    "        print(\"Empty note...\",index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try plotting these times as a histogram (hint, save the runtimes right in the notes dataframe...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
