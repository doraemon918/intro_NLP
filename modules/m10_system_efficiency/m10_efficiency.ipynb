{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Efficiency...the clock matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In Module 7 we discussed NLP system performance in terms of the *effectiveness* of an NLP system compared to human annotations. We focused on measures like recall, precision, and accuracy. In this notebook we will explore another measure of NLP systems, system *efficiency*. Testing an NLP system's efficiency asks, \"How fast is it?\" Or, put another, more practical way, \"Is the system *fast enough?*\"\n",
    "\n",
    "In the real world of clinical NLP, increasingly we want to process ever more notes. NLP technology, and the hardware that supports it, improved dramatically over the past 15 years. Critically, hardware costs have plummeted at the same time as the HITECH Act made vast quantities of clinical data easier to create and exchange.\n",
    "\n",
    "But as we process more and more notes we need to be mindful of how long it takes to do so. Processing efficiency becomes increasingly important as the size of clinical datasets grows. In clinical natural language processing (NLP), even small increases in processing throughput are important when handling very large note corpora. \n",
    "### The Clock on the Wall\n",
    "An extreme example is the U.S. Veterans Administration VINCI national data warehouse, which contains 2-3 billion notes. Divita et al. demonstrated the effect of note processing efficiency in VINCI studies (see the required article for this Module). In their work, 6 million records is considered a representative national sample for many applications. Shaving off 100 milliseconds (i.e., one-tenth of second) of processing time per note in their benchmark system (401 milliseconds was their nominal per-note-processing time) **would save nearly a week** of clock time for a corpus of that size.\n",
    "\n",
    "To assess efficiency, we focus on “clock time,” the amount of real time, from the human perspective, a process takes to complete. To put clock time into a *real-world context, see the real-time, near real-time, and daily examples* in the Module 10 lecture. In this notebook, let’s explore the scenario (a very practical one as you start to build your project!) where you, as a researcher, are tweaking an NLP algorithm and have to wait for processing after each tweak.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `timeit` library\n",
    "First we need some Python tools that can provide a stopwatch for measuring how long Python processes take to run. These will allow us to see where clock-time bottlenecks are. For more information on the timeit library, [look here in the Python documention.](https://docs.python.org/2/library/timeit.html#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, let's import what we need...\n",
    "import timeit\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a test function that we can use to practice:\n",
    "def test():\n",
    "    '''A test function that builds a list of the first 10,000 integers by appending them -- nothing fancy'''\n",
    "    mylist = []\n",
    "    for i in range(100000):\n",
    "        mylist.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Timeit` provides a new class and a few methods to measure the time it takes to execute statements. To keep things simple, we won't worry about the class business now, just focus on the methods. One useful method is: \n",
    "\n",
    "`timeit.repeat(stmt = '<the Python code statement you intend to time>', \\\n",
    "    setup= '<any Python code to run each time the REPEAT below starts>' \\  #optional\n",
    "    repeat = <number>, \\ #i.e., how many times to REPEAT the number of runs below AND average the results \\\n",
    "    number = <number>) #how many times to run the stmt code, i.e.,the TOTAL runs is (repeat*number)`\n",
    "\n",
    "In the first example, we will run test(), which appends 10,000 times, for 10 batches (i.e., `number = 10`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... The default repeat is 3, so 30 runs of test(), or **300,000** appends total\n",
      " for a series of  [0.15647347690537572, 0.09506166819483042, 0.09126702509820461]  seconds.\n"
     ]
    }
   ],
   "source": [
    "#1) Run test() in loops of 10, average those, and repeat 3 times (the default if repeat is not specified):\n",
    "runtime = timeit.repeat(stmt = 'test()', \n",
    "                        setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                        number = 10)\n",
    "print(' The default repeat is 3, so 30 runs of test(), or **300,000** appends total\\n for a series of ', \\\n",
    "      runtime, ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pretty fast. \n",
    "\n",
    "How may 100ths of seconds, on average, did the three runs take to append 10,000 times in`test()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.114267390066 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(runtime),\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Repeat 10 batches of 100 runs and average the times for batch, **10,000,000** appends total\n",
      " for a series of  [1.013889370020479, 0.9408897641114891, 0.9301630049012601, 0.9401369169354439, 0.9117680923081934, 0.9292277037166059, 0.9321360560134053, 0.9311231565661728, 0.9579355088062584, 0.929843776859343]  seconds.\n"
     ]
    }
   ],
   "source": [
    "#2) Now run test() in batches of 100 and repeat 10 times, then average the time for each repeat loop\n",
    "runtime = (timeit.repeat(stmt = 'test()', \n",
    "                         setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                         repeat = 10, \n",
    "                         number = 100))\n",
    "print('Repeat 10 batches of 100 runs and average the times for batch, **10,000,000** appends total\\n for a series of ', runtime, ' seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941711335024 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(runtime),\"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, pretty fast. Compare the overall average for both runs above...is it what you expected? \n",
    "\n",
    "Run the two cells above again. You will see that the mean time is slightly different. **Why is that?** Because the CPU is multitasking. If this Python notebook was the only process running on the CPU, the times would be much more consistent. But you are sharing the CPU with the rest of the class and with everyone else using the server. And, of course, the server has several tasks of its own to do. **That is why, when timing code, it's important to average over several runs to smooth out the interference from other processes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> WAIT for it!\n",
      "...\n",
      "The average time to append 10,000 integers 1,000 times (from 3 repeats) is  9.34650781921 seconds\n"
     ]
    }
   ],
   "source": [
    "# 3) Okay, now let's increase the number of runs one more time: Run test() in batches of 1,000, average \\\n",
    "# the results, and repeat 5 times:\n",
    "print(\"--> WAIT for it!\")\n",
    "runtime = timeit.repeat(stmt = 'test()', \n",
    "                    setup = 'print(\".\", end = \"\"); from __main__ import test', #print a period after each repeat\n",
    "                    repeat = 3, \n",
    "                    number = 1000)\n",
    "print(\"\\nThe average time to append 10,000 integers 1,000 times (from 3 repeats) is \", np.mean(runtime), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, do these execution times for the three examples seem about what you expected? **It is reassuringly linear**, that is, every time we increase the number of runs by a factor of 10 the execution time also increases roughly by a factor of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets time some NLP processes...\n",
    "One annoying feature of `timeit` is that it really only wants to clock a single statement. That means that you need to put the code that you want to time inside of a function. It also means you need to `import` that function's parameters, as shown below. *If you find a way to time multiple statements in timeit, do let the class know! The documentation implies it is possible, but it is not clear to us how....* \n",
    "\n",
    "We are going to pinch code from the implementation notenook back in Module 7, the code that segments a note and then runs the PyConText assertion process on those segments. **Note:** in your project, and in the real world, you would be processing hundreds or thousands of notes in a typical pipeline run. We *simulate* that in this exercise by timing the pipeline as it runs on one note hundred of times, over and over. This is not a brilliant approach to simulation, because any given note might be particularly easy or particularly hard for segmentation and context assertion. \n",
    "\n",
    "**At the end of the notebook, if you are you feeling adventurous, pinch the code from an earlier notebook, read in a hundred notes, and loop over them with the timer. :) If you do that, you will notice a decidedly different mean runtime per note.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the usual imports\n",
    "from pipeUtils import Document\n",
    "from pipeUtils import Annotation\n",
    "\n",
    "from wrapperPyConText import ConTextPipe\n",
    "from PyRuSH.RuSH import RuSH\n",
    "\n",
    "#Now create a document instance and populate it with grab a single note and its annotations\n",
    "doc = Document() #create a document instance\n",
    "doc.load_document_from_file('data/test.txt') #use the document methods to read in the note and its annotations\n",
    "doc.load_annotations_from_brat('data/test.ann')\n",
    "\n",
    "#Next, read in the sementer rule set, in the KB directory, and create a sentence segmenter instance\n",
    "sentence_rules= 'KB/rush_rules.tsv'\n",
    "sentence_segmenter = RuSH(sentence_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information extraction rule set below is teeny. This is really a toy set of rules; a real pipeline will have far more rules and thus will run more slowly than our experiments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the information extraction rules. This is a toy set, a real pipeline will have many ruch rules.\n",
    "#and thus a real pipeline will be slower.\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: pain\n",
    "Regex: ''\n",
    "Type: Depression_Symptom\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: depression\n",
    "Regex: 'depres\\\\w+'\n",
    "Type: Depression_Symptom\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: suicidal\n",
    "Regex: 'suicidal\\\\s*ideation'\n",
    "Type: Depression_Symptom\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `timeit`, we need to wrap the segmenter and context code into a function we can time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_anns(document):\n",
    "    '''A test function that wrap a document segmenter and context asserter so we can time the runtime of both.'''\n",
    "    #segment the document's sentences\n",
    "    sentences=sentence_segmenter.segToSentenceSpans(doc.text)\n",
    "    i=0\n",
    "    for sentence in sentences: \n",
    "        i=i+1  #incrementing the index\n",
    "        doc.annotations.append(Annotation(start_index=sentence.begin,\n",
    "                                          end_index=sentence.end, \n",
    "                                          type=\"Sentence\", \n",
    "                                          ann_id='Sent_'+str(i)))\n",
    "    #now run context on each sentence segment\n",
    "    context_pipe = ConTextPipe(target_rules, rule_set)\n",
    "    doc.annotations = [] #a subtle 'persistence' side-effect here; if not reset it accumulates across runs\n",
    "    for a in doc.annotations: #loop through the sentences and run the context algorithm over each\n",
    "        if a.type == \"Sentence\":\n",
    "            b = context_pipe.process(a, doc.text)\n",
    "            doc.annotations.extend(b)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set to time the new \"segmenter-context\" function. We will make three runs, each time adding more context rules. More rules mean more *effective* context assertion, but that comes at the price of lower efficiencey (slower performance), as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Time(s) for rule set  KB/general_modifiers_50-rules.yml for 10 notes:  [1.6463442277163267, 1.5755672696977854, 1.591118745971471]\n",
      "...Time(s) for rule set  KB/general_modifiers_half-rules.yml for 10 notes:  [2.702311024069786, 2.6901290197856724, 2.7027526618912816]\n",
      "...Time(s) for rule set  KB/general_modifiers_all-rules.yml for 10 notes:  [4.270881077740341, 4.2291063903830945, 4.3078813180327415]\n"
     ]
    }
   ],
   "source": [
    "rule_sets = ['KB/general_modifiers_50-rules.yml', \\\n",
    "             'KB/general_modifiers_half-rules.yml', \\\n",
    "             'KB/general_modifiers_all-rules.yml']\n",
    "\n",
    "for rule_set in rule_sets:\n",
    "    runtime = timeit.repeat('do_anns(doc)',\n",
    "                            setup = 'print(\".\", end = \"\"); from __main__ import do_anns, doc', #ugh...note the import\n",
    "                            repeat = 3, \n",
    "                            number = 10) ##simulates a 10 note run\n",
    "    print(\"Time(s) for rule set \",rule_set,\"for 10 notes: \", runtime)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: option -s not recognized ( allowed: \"n:r:tcp:qo\" )\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10 -n 1 -p 5 #-s 'print(\".\", end = \"\")'\n",
    "rule_sets = ['KB/general_modifiers_all-rules.yml']\n",
    "\n",
    "for rule_set in rule_sets:\n",
    "    sentences=sentence_segmenter.segToSentenceSpans(doc.text)\n",
    "    i=0\n",
    "    for sentence in sentences: \n",
    "        i=i+1  #incrementing the index\n",
    "        doc.annotations.append(Annotation(start_index=sentence.begin,\n",
    "                                          end_index=sentence.end, \n",
    "                                          type=\"Sentence\", \n",
    "                                          ann_id='Sent_'+str(i)))\n",
    "    #now run context on each sentence segment\n",
    "    context_pipe = ConTextPipe(target_rules, rule_set)\n",
    "    doc.annotations = [] #a subtle 'persistence' side-effect here; if not reset it accumulates across runs\n",
    "    for a in doc.annotations: #loop through the sentences and run the context algorithm over each\n",
    "        if a.type == \"Sentence\":\n",
    "            b = context_pipe.process(a, doc.text)\n",
    "            doc.annotations.extend(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The \"half-rules.yml\" context rule set has about 190 rules, while the \"all-rules.yml\" set has about 380 rules. Yet these runs times are **not so linear**. Why is that, do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Try reading a hundred notes from MIMIC II and timing the pipeline here on each note, rather than simply procesing the same note repeatedly like we do above. To ge you started, here's the SQL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter MySQL passwd for jovyan········\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_sql(\"SELECT DISTINCT * FROM noteevents limit 100\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE YOUR timer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1000 -n 10 -p 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    \\n \\n \\n \\nAdmission Date:  [**2644-1-17**]   ...\n",
      "1    \\n\\n\\n     DATE: [**2644-1-17**] 10:53 AM\\n   ...\n",
      "2    \\n\\n\\n     DATE: [**2644-1-17**] 10:43 AM\\n   ...\n",
      "3    \\n\\n\\n     DATE: [**2644-1-17**] 6:37 AM\\n    ...\n",
      "4    \\nNSG Admit noteB:\\nPlease refer to careview a...\n",
      "5    \\nNursing Progress Note:\\nPlease refer to Care...\n",
      "Name: text, dtype: object\n",
      "Time(s) for note  0 :  [0.8701527533121407]\n",
      "Time(s) for note  1 :  [0.47419741470366716]\n",
      "Time(s) for note  2 :  [0.4613146777264774]\n",
      "Time(s) for note  3 :  [0.4774014907889068]\n",
      "Time(s) for note  4 :  [0.41591173131018877]\n",
      "Time(s) for note  5 :  [0.4160619741305709]\n",
      "Time(s) for note  6 :  [0.42232962930575013]\n",
      "Time(s) for note  7 :  [0.3776710261590779]\n",
      "Time(s) for note  8 :  [0.47029974590986967]\n",
      "Time(s) for note  9 :  [1.2523656389676034]\n",
      "Time(s) for note  10 :  [0.4285039622336626]\n",
      "Time(s) for note  11 :  [0.41606609942391515]\n",
      "Time(s) for note  12 :  [0.45940754003822803]\n",
      "Time(s) for note  13 :  [0.48897488275542855]\n",
      "Time(s) for note  14 :  [0.41830643080174923]\n",
      "Time(s) for note  15 :  [0.42011357098817825]\n",
      "Time(s) for note  16 :  [0.43509013717994094]\n",
      "Time(s) for note  17 :  [0.44436965929344296]\n",
      "Time(s) for note  18 :  [0.37565224105492234]\n",
      "Time(s) for note  19 :  [0.4856409369967878]\n",
      "Time(s) for note  20 :  [0.422836528159678]\n",
      "Time(s) for note  21 :  [2.2036810410209]\n",
      "Empty note... 22\n",
      "Time(s) for note  23 :  [0.42224264796823263]\n",
      "Time(s) for note  24 :  [0.385868732817471]\n",
      "Time(s) for note  25 :  [0.4749535289593041]\n",
      "Time(s) for note  26 :  [0.3383916667662561]\n",
      "Time(s) for note  27 :  [0.8550816439092159]\n",
      "Time(s) for note  28 :  [0.4207545812241733]\n",
      "Time(s) for note  29 :  [0.4682723400183022]\n",
      "Time(s) for note  30 :  [0.3940766560845077]\n",
      "Time(s) for note  31 :  [0.3711526063270867]\n",
      "Time(s) for note  32 :  [0.4335938128642738]\n",
      "Time(s) for note  33 :  [0.4617602280341089]\n",
      "Time(s) for note  34 :  [0.4027281831949949]\n",
      "Time(s) for note  35 :  [0.7008159146644175]\n",
      "Time(s) for note  36 :  [0.33253659727051854]\n",
      "Time(s) for note  37 :  [0.3324225409887731]\n",
      "Time(s) for note  38 :  [0.39128942461684346]\n",
      "Time(s) for note  39 :  [0.39255635533481836]\n",
      "Time(s) for note  40 :  [0.3334047319367528]\n",
      "Time(s) for note  41 :  [0.4051925209350884]\n",
      "Time(s) for note  42 :  [0.3400035873055458]\n",
      "Time(s) for note  43 :  [0.4175570532679558]\n",
      "Time(s) for note  44 :  [0.35607157880440354]\n",
      "Time(s) for note  45 :  [0.3781644459813833]\n",
      "Time(s) for note  46 :  [0.35364993289113045]\n",
      "Time(s) for note  47 :  [0.38397858710959554]\n",
      "Time(s) for note  48 :  [0.3374874792061746]\n",
      "Time(s) for note  49 :  [0.3861631779000163]\n",
      "Time(s) for note  50 :  [0.41482394095510244]\n",
      "Time(s) for note  51 :  [0.3869160641916096]\n",
      "Time(s) for note  52 :  [0.33059839764609933]\n",
      "Time(s) for note  53 :  [0.4296819348819554]\n",
      "Time(s) for note  54 :  [0.3625629269517958]\n",
      "Time(s) for note  55 :  [0.38984303129836917]\n",
      "Time(s) for note  56 :  [0.3625116520561278]\n",
      "Time(s) for note  57 :  [0.37387515511363745]\n",
      "Time(s) for note  58 :  [0.379567404743284]\n",
      "Time(s) for note  59 :  [1.0232476857490838]\n",
      "Time(s) for note  60 :  [0.4283769950270653]\n",
      "Time(s) for note  61 :  [0.42526631569489837]\n",
      "Time(s) for note  62 :  [0.41479141917079687]\n",
      "Time(s) for note  63 :  [0.40185145381838083]\n",
      "Time(s) for note  64 :  [0.40816938877105713]\n",
      "Time(s) for note  65 :  [0.5751645648851991]\n",
      "Time(s) for note  66 :  [0.43502617394551635]\n",
      "Time(s) for note  67 :  [0.4118791581131518]\n",
      "Time(s) for note  68 :  [0.4121510302647948]\n",
      "Time(s) for note  69 :  [0.35071255825459957]\n",
      "Time(s) for note  70 :  [0.47772420989349484]\n",
      "Time(s) for note  71 :  [0.368730659596622]\n",
      "Time(s) for note  72 :  [0.43205000227317214]\n",
      "Time(s) for note  73 :  [0.4627231601625681]\n",
      "Time(s) for note  74 :  [0.34344278601929545]\n",
      "Time(s) for note  75 :  [0.40336313704028726]\n",
      "Time(s) for note  76 :  [0.49042227305471897]\n",
      "Time(s) for note  77 :  [0.4770403290167451]\n",
      "Time(s) for note  78 :  [0.4817441632039845]\n",
      "Time(s) for note  79 :  [0.5896314289420843]\n",
      "Time(s) for note  80 :  [0.7057100343517959]\n",
      "Time(s) for note  81 :  [0.534736008848995]\n",
      "Time(s) for note  82 :  [0.4298912570811808]\n",
      "Time(s) for note  83 :  [0.5744824479334056]\n",
      "Time(s) for note  84 :  [0.5244384198449552]\n",
      "Time(s) for note  85 :  [0.43098126258701086]\n",
      "Time(s) for note  86 :  [0.5065960949286819]\n",
      "Time(s) for note  87 :  [0.45271155424416065]\n",
      "Time(s) for note  88 :  [0.49712969502434134]\n",
      "Time(s) for note  89 :  [0.606309587135911]\n",
      "Time(s) for note  90 :  [0.4111919510178268]\n",
      "Time(s) for note  91 :  [0.39278418803587556]\n",
      "Time(s) for note  92 :  [0.6050805477425456]\n",
      "Time(s) for note  93 :  [0.5489237527363002]\n",
      "Time(s) for note  94 :  [0.5243027578108013]\n",
      "Time(s) for note  95 :  [0.675012415740639]\n",
      "Time(s) for note  96 :  [0.4356137802824378]\n",
      "Time(s) for note  97 :  [0.4225418111309409]\n",
      "Time(s) for note  98 :  [0.48639157274737954]\n",
      "Time(s) for note  99 :  [0.6760410820133984]\n"
     ]
    }
   ],
   "source": [
    "#Here's ours:\n",
    "print(notes.text[0:6])\n",
    "rule_set = 'KB/general_modifiers_all-rules.yml'\n",
    "for index, note in notes.iterrows():\n",
    "    new_note = note.text.replace('\\n',' ')\n",
    "    doc = Document()\n",
    "    doc.text = new_note  #sets up the text for the next note to run throgh the pipe\n",
    "    doc.load_annotations_from_brat('data/test.ann')#since we don't have annotations yet, just use the test annotations\n",
    "    if re.match(r'(\\s*)(\\w+)', doc.text) != None:   #make sure the note has characters in it, else RUSH could crash\n",
    "        runtime = timeit.repeat('do_anns(doc)',\n",
    "                                setup = 'from __main__ import do_anns, doc', #ugh...note the import\n",
    "                                repeat = 1, \n",
    "                                number = 1)\n",
    "        print(\"Time(s) for note \",index,\": \", runtime)\n",
    "    else:\n",
    "        print(\"Empty note...\",index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try plotting these times as a histogram (hint, save the runtimes right in the notes dataframe...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
