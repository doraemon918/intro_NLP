{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "In class we have gone through a rule-based NLP pipeline by executing components one by one. Typically, we create a pipeline as a single class that links all modules.\n",
    "\n",
    "In this exercise, you will need to write a pipeline class. Let's call it **MyPipe**, it can be initiated through taking a set of rules. After that, it can be called through a **process** function, which can take a document text, and output a set of values: \n",
    "- document level classification\n",
    "- context document, which is an object of type pyConTextGraph.ConTextDocument;\n",
    "- annotations (in dataframe format);\n",
    "- relations (in dataframe format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state your import here\n",
    "\n",
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF\n",
    "\n",
    "\n",
    "# begin to define MyPipe class\n",
    "class MyPipe:\n",
    "    def __init__(self, sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule):\n",
    "        # initiate necessary components here\n",
    "        self.sentence_rules = sentence_rules\n",
    "        self.target_rules = target_rules\n",
    "        self.context_rules = context_rules\n",
    "        self.feature_inference_rule = feature_inference_rule\n",
    "        self.document_inference_rule = document_inference_rule\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def process(self, doc_text):        \n",
    "        #process your input doc_text, return the required results\n",
    "        self.doc_text = doc_text\n",
    "        \n",
    "        \n",
    "        \n",
    "        return doc_class, context_doc, annotations, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your pipeline class is defined, you can use it multiple times for different set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Now you can select documents from the MIMIC database (limit to 5 documents that contain your target concept), write a script to process all of them, and output a dictionary which uses document name as keys and document level classification as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure your rules \n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: fever\n",
    "Regex: ''\n",
    "Type: FEVER\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: high temperature\n",
    "Regex: '1\\d\\d\\.\\d F'\n",
    "Type: FEVER'''\n",
    "context_rules='''\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'no'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'denies'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "FEVER_DOC,FEVER\n",
    "\n",
    "#Default document type\n",
    "NO_FEVER\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate an instance of MyPipe\n",
    "myPipe=MyPipe(sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write you code here, fill the results by processing each document through MyPipe\n",
    "results=dict()  # this dictionary will contain document names as keys and a document-level classification as values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Now you get the results, but how can you be sure if they are correct? Let's dig a little deeper to visualize them. \n",
    "\n",
    "Hint: **view_pycontext_output** can take in a list of ConTextDocuments and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the context documents that you created in Exercise 2.\n",
    "context_docs=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the file name to contain your UNID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'getDocument'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0c18e385675d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mview_pycontext_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"u1166466.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/WORK/intro_NLP/modules/m5_rule_based_nlp/visual.py\u001b[0m in \u001b[0;36mview_pycontext_output\u001b[0;34m(input, vis)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mview_pycontext_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mview_pycontext_single_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WORK/intro_NLP/modules/m5_rule_based_nlp/visual.py\u001b[0m in \u001b[0;36mview_pycontext_single_output\u001b[0;34m(processed_doc, vis)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mview_pycontext_single_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KB/fam_modifiers.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_html_from_context_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     html = '''\n",
      "\u001b[0;32m~/WORK/intro_NLP/modules/m5_rule_based_nlp/visual.py\u001b[0m in \u001b[0;36mgen_html_from_context_doc\u001b[0;34m(self, doc, filter_no_markup_txt)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_html_from_context_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_no_markup_txt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertMarkups2DF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_document_markups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_html_from_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_txt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyConTextNLP/utils.py\u001b[0m in \u001b[0;36mget_document_markups\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_document_markups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\" Given a ConTextDocument return an ordered list of the ConTextmarkup objects consistituting the document\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     tmp = [(e[1],e[2]['sentenceNumber']) for e in document.getDocument().edges(data=True) if\n\u001b[0m\u001b[1;32m      6\u001b[0m            e[2].get('category') == 'markup']\n\u001b[1;32m      7\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'getDocument'"
     ]
    }
   ],
   "source": [
    "view_pycontext_output(context_docs, Vis(file_name=\"u1166466.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please make a screenshot of the visualization file and submit it as homework assignment.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
