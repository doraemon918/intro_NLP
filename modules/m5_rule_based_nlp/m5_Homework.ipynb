{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "In class we have gone through a rule-based NLP pipeline by executing components one by one. Typically, we create a pipeline as a single class that links all modules.\n",
    "\n",
    "In this exercise, you will need to write a pipeline class. Let's call it **MyPipe**, it can be initiated through taking a set of rules. After that, it can be called through a **process** function, which can take a document text, and output a set of values: \n",
    "- document level classification\n",
    "- context document, which is an object of type pyConTextGraph.ConTextDocument;\n",
    "- annotations (in dataframe format);\n",
    "- relations (in dataframe format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state your import here\n",
    "\n",
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF\n",
    "\n",
    "\n",
    "# begin to define MyPipe class\n",
    "class MyPipe:\n",
    "    def __init__(self, sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule):\n",
    "        # initiate necessary components here\n",
    "        \n",
    "        self.sentence_segmenter = RuSH(sentence_rules)\n",
    "        self.feature_inferencer=FeatureInferencer(feature_inference_rule)\n",
    "        self.document_inferencer = DocumentInferencer(document_inference_rule)\n",
    "        self.targets=get_item_data(target_rules)\n",
    "        self.modifiers=get_item_data(context_rules)\n",
    "        #pass\n",
    "    \n",
    "    def process(self, doc_text):\n",
    "        #process your input doc_text, return the required results\n",
    "        \n",
    "        sentences = self.sentence_segmenter.segToSentenceSpans(doc_text)\n",
    "        \n",
    "        context_doc = pyConTextGraph.ConTextDocument()\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_text=doc_text[sentence.begin:sentence.end].lower()\n",
    "            # Process every sentence by adding markup\n",
    "            m = markup_sentence(sentence_text, modifiers=self.modifiers, targets=self.targets)\n",
    "            context_doc.addMarkup(m)\n",
    "            context_doc.getSectionMarkups()\n",
    "            #print(m)\n",
    "        markups = get_document_markups(context_doc)\n",
    "        annotations, relations, doc_txt = convertMarkups2DF(markups) \n",
    "        \n",
    "        inferenced_types = self.feature_inferencer.process(annotations, relations)\n",
    "        doc_class = self.document_inferencer.process(inferenced_types)\n",
    "        \n",
    "        return doc_class, context_doc, annotations, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your pipeline class is defined, you can use it multiple times for different set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Now you can select documents from the MIMIC database (limit to 5 documents that contain your target concept), write a script to process all of them, and output a dictionary which uses document name as keys and document level classification as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql       #imports the Python mysql module\n",
    "import pandas as pd  #imports the Python data analysis library \n",
    "import getpass       #imports the getpass module\n",
    "from nltk.tokenize import sent_tokenize       #imports a sentence splitter\n",
    "from nltk.tokenize import word_tokenize       #imports a string tokenizer breaking on whitespace\n",
    "from nltk.tokenize import wordpunct_tokenize  #imports a string tokenizer breaking on whitespace and punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer #used for generating word spans later\n",
    "from nltk import pos_tag                      #a part-of-speech tagger\n",
    "import re                                     #imports Python's regular expression functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter MySQL passwd for user jovyan········\n"
     ]
    }
   ],
   "source": [
    "dbconn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for user jovyan\"),db='mimic2')\n",
    "cursor = dbconn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(text)\n",
      "0         9591\n"
     ]
    }
   ],
   "source": [
    "tables = pd.read_sql(\"SELECT table_name FROM information_schema.tables where table_schema='mimic2'\", dbconn)\n",
    "#print(tables)\n",
    "print(pd.read_sql(\"SELECT count(text) from noteevents WHERE text like '%fever%' LIMIT 5\",dbconn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_text = pd.read_sql(\"SELECT subject_id, text from noteevents   where text like '% fever %' order by rand() limit  5\",dbconn)  \n",
    "\n",
    "#add each new note to a list of notes as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there are too many sentence segmentation rules, let's read them from an external file\n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "# you can point target_rules to a file path instead, if there are many rules\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: fever\n",
    "Regex: ''\n",
    "Type: FEVER\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: high temperature\n",
    "Regex: '1\\d\\d\\.\\d F'\n",
    "Type: FEVER'''\n",
    "# context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "context_rules='''Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'no'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'denies'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the feature inference rule\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the document inference rule\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "FEVER_DOC,FEVER\n",
    "\n",
    "#Default document type\n",
    "NO_FEVER\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate an instance of MyPipe\n",
    "myPipe=MyPipe(sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyPipe at 0x7f7091f02e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write you code here, fill the results by processing each document through MyPipe\n",
    "results=dict()  # this dictionary will contain document names as keys and a document-level classification as values.\n",
    "for index, row in doc_text.iterrows():\n",
    "    #row[1].replace('\\n','')\n",
    "    results[index] = myPipe.process(row[1].replace('\\n','')) \n",
    "    #print(index, row[0], row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fever_doc', __________________________________________\n",
      ",   markup_id vis_category  start    end    txt                        type\n",
      "0        T0       Target  277.0  282.0  fever                       fever\n",
      "1        T1       Target  583.0  588.0  fever                       fever\n",
      "2        T2     Modifier  398.0  400.0     no  definite_negated_existence,   relation_id                        type arg1_cate arg1_id arg2_cate arg2_id\n",
      "0          R0  definite_negated_existence  Modifier      T2    Target      T1)\n"
     ]
    }
   ],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Now you get the results, but how can you be sure if they are correct? Let's dig a little deeper to visualize them. \n",
    "\n",
    "Hint: **view_pycontext_output** can take in a list of ConTextDocuments and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import view_pycontext_outputs\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the context documents that you created in Exercise 2.\n",
<<<<<<< HEAD
    "context_docs=dict()\n",
    "\n",
    "doc = results[1]\n",
    "doc1 = doc[1]\n"
=======
    "# Create a dictionary with a document name as the key and ConText documents as the value \n",
    "# for all documents in the results\n",
    "context_docs=dict()\n"
>>>>>>> 2ca4523c2bd2e5338818c5a35e3f1f4e2d0585b9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t      <iframe src = \"tmp/default.html\" frameborder=\"0\" width = \"850\" height = \"275\">\n",
       "\t\t\t         Sorry your browser does not support inline frames.\n",
       "\t\t\t      </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_pycontext_output(doc1)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize all documents\n",
    "view_pycontext_outputs(context_docs)"
>>>>>>> 2ca4523c2bd2e5338818c5a35e3f1f4e2d0585b9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the file name to contain your UNID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t      <iframe src = \"tmp/u1166466.html\" frameborder=\"0\" width = \"850\" height = \"275\">\n",
       "\t\t\t         Sorry your browser does not support inline frames.\n",
       "\t\t\t      </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
<<<<<<< HEAD
    "view_pycontext_output(doc[1], Vis(file_name=\"u1166466.html\"))"
=======
    "# Pick one of the documents that has negation and visualize it for the final output\n",
    "view_pycontext_output(context_doc[2], Vis(file_name=\"YOUR_UNID.html\"))"
>>>>>>> 2ca4523c2bd2e5338818c5a35e3f1f4e2d0585b9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please make a screenshot of the visualization file and submit it as homework assignment.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
