{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from DocumentClassifier import DocumentClassifier \n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF\n",
    "\n",
    "\n",
    "# begin to define MyPipe class\n",
    "class MyPipe:\n",
    "    def __init__(self, sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule):\n",
    "        # initiate necessary components here        \n",
    "        self.sentence_segmenter = RuSH(sentence_rules)\n",
    "        self.targets=get_item_data(target_rules)\n",
    "        self.modifiers=get_item_data(context_rules)\n",
    "        self.feature_inferencer=FeatureInferencer(feature_inference_rule)\n",
    "        self.document_inferencer = DocumentInferencer(document_inference_rule)\n",
    "    \n",
    "    def process(self, doc_text):        \n",
    "        #process your input doc_text, return the required results\n",
    "        sentences=self.sentence_segmenter.segToSentenceSpans(doc_text)\n",
    "        #sentences=doc_text.split('\\n')\n",
    "        context_doc = pyConTextGraph.ConTextDocument()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_text=doc_text[sentence.begin:sentence.end].lower()\n",
    "            # Process every sentence by adding markup\n",
    "            m = markup_sentence(sentence_text, modifiers=self.modifiers, targets=self.targets)\n",
    "            context_doc.addMarkup(m)\n",
    "            #context_doc.getSectionMarkups()\n",
    "        \n",
    "        markups = get_document_markups(context_doc)\n",
    "        annotations, relations, doc_txt = convertMarkups2DF(markups) \n",
    "        \n",
    "        inferenced_types = self.feature_inferencer.process(annotations, relations)\n",
    "        doc_class = self.document_inferencer.process(inferenced_types)\n",
    "        \n",
    "        return doc_class, context_doc, annotations, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure your rules \n",
    "\n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "# you can point target_rules to a file path instead, if there are many rules\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: fever\n",
    "Regex: ''\n",
    "Type: FEVER\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: high temperature\n",
    "Regex: '1\\d\\d\\.\\d F'\n",
    "Type: FEVER'''\n",
    "# context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "context_rules='''Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'no'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'denies'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the feature inference rule\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the document inference rule\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "FEVER_DOC,FEVER\n",
    "\n",
    "#Default document type\n",
    "NO_FEVER\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate an instance of MyPipe\n",
    "myPipe=MyPipe(sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_data = pd.read_sql(\"SELECT `text` FROM noteevents where `text` like '%fever%' limit 5\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write you code here, fill the results by processing each document through MyPipe\n",
    "results=dict()  # this dictionary will contain document names as keys and result as values.\n",
    "for index, row in pad_data.iterrows():\n",
    "    t = row.text.replace('\\n',' ')\n",
    "    result = myPipe.process(t)\n",
    "    results[index] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import view_pycontext_outputs\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the context documents that you created in Exercise 2 and put them into a dictionary.\n",
    "context_docs=dict()\n",
    "i=0\n",
    "for item, row in results.items(): \n",
    "    doc_name = \"doc\" + str(i)\n",
    "    context_docs[doc_name]=row[1]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_pycontext_output(context_docs['doc1'], Vis(file_name=\"test.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_pycontext_outputs(context_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
