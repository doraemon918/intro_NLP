{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI 6115 Module 3: Pipelines, where the different levels of processing come together\n",
    "review of downloading and using useful NLTK functions\n",
    "review of accessing the MIMIC II deceased data set\n",
    "reading a small number of notes and processing for POS (including histograms and parse trees)\n",
    "example of crude search across the MIMIC II deceased data set for keywords in the Module to use case of peripheral artery disease (meant to show them how to get a rough idea how many notes might contain some target concepts of interest to stu\n",
    "   \n",
    "If you want to use the techniques of biomedical text processing to accomplish some goal, then typically you start with some corpus (a collection of texts of interest to you) and and then process it in some project-specific way. A useful way to think about your project is to conceive of it as a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple NLP general purpose pipeline\n",
    "![Pipleine graphic](../../media/m3_levels_of_processing/simple_pipeline_final.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP pipeline paradigm\n",
    "Almost all NLP systems that do something useful use some form of a *pipeline*. Like any good programming system, a pipeline breaks a big problem into small, manageable tasks. The pipeline shown above has four tasks and these particular tasks are very common in NLP. We will cover pipeline design in more detail later in the course. But for now, note that in Python the simplest pipeline is just a program with several defined functions that are called in order.\n",
    "\n",
    "**This in-class notebook will run through some common taks that make up parts of common pipeline components.**\n",
    "\n",
    "To get started, let's import what we need. If you have worked through the NLTK_basics notebook as required in Module 1, then the `NLTK` commands below should already be available for importing. BUT NOTE: if we had to rebuild the JupyterHub system to add content then you need to re-load the`NLTK` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> book\n",
      "    Downloading collection 'book'\n",
      "       | \n",
      "       | Downloading package abc to /home/u1166466/nltk_data...\n",
      "       |   Package abc is already up-to-date!\n",
      "       | Downloading package brown to /home/u1166466/nltk_data...\n",
      "       |   Package brown is already up-to-date!\n",
      "       | Downloading package chat80 to /home/u1166466/nltk_data...\n",
      "       |   Package chat80 is already up-to-date!\n",
      "       | Downloading package cmudict to /home/u1166466/nltk_data...\n",
      "       |   Package cmudict is already up-to-date!\n",
      "       | Downloading package conll2000 to /home/u1166466/nltk_data...\n",
      "       |   Package conll2000 is already up-to-date!\n",
      "       | Downloading package conll2002 to /home/u1166466/nltk_data...\n",
      "       |   Package conll2002 is already up-to-date!\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package dependency_treebank is already up-to-date!\n",
      "       | Downloading package genesis to /home/u1166466/nltk_data...\n",
      "       |   Package genesis is already up-to-date!\n",
      "       | Downloading package gutenberg to /home/u1166466/nltk_data...\n",
      "       |   Package gutenberg is already up-to-date!\n",
      "       | Downloading package ieer to /home/u1166466/nltk_data...\n",
      "       |   Package ieer is already up-to-date!\n",
      "       | Downloading package inaugural to /home/u1166466/nltk_data...\n",
      "       |   Package inaugural is already up-to-date!\n",
      "       | Downloading package movie_reviews to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package movie_reviews is already up-to-date!\n",
      "       | Downloading package nps_chat to /home/u1166466/nltk_data...\n",
      "       |   Package nps_chat is already up-to-date!\n",
      "       | Downloading package names to /home/u1166466/nltk_data...\n",
      "       |   Package names is already up-to-date!\n",
      "       | Downloading package ppattach to /home/u1166466/nltk_data...\n",
      "       |   Package ppattach is already up-to-date!\n",
      "       | Downloading package reuters to /home/u1166466/nltk_data...\n",
      "       |   Package reuters is already up-to-date!\n",
      "       | Downloading package senseval to /home/u1166466/nltk_data...\n",
      "       |   Package senseval is already up-to-date!\n",
      "       | Downloading package state_union to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package state_union is already up-to-date!\n",
      "       | Downloading package stopwords to /home/u1166466/nltk_data...\n",
      "       |   Package stopwords is already up-to-date!\n",
      "       | Downloading package swadesh to /home/u1166466/nltk_data...\n",
      "       |   Package swadesh is already up-to-date!\n",
      "       | Downloading package timit to /home/u1166466/nltk_data...\n",
      "       |   Package timit is already up-to-date!\n",
      "       | Downloading package treebank to /home/u1166466/nltk_data...\n",
      "       |   Package treebank is already up-to-date!\n",
      "       | Downloading package toolbox to /home/u1166466/nltk_data...\n",
      "       |   Package toolbox is already up-to-date!\n",
      "       | Downloading package udhr to /home/u1166466/nltk_data...\n",
      "       |   Package udhr is already up-to-date!\n",
      "       | Downloading package udhr2 to /home/u1166466/nltk_data...\n",
      "       |   Package udhr2 is already up-to-date!\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package unicode_samples is already up-to-date!\n",
      "       | Downloading package webtext to /home/u1166466/nltk_data...\n",
      "       |   Package webtext is already up-to-date!\n",
      "       | Downloading package wordnet to /home/u1166466/nltk_data...\n",
      "       |   Package wordnet is already up-to-date!\n",
      "       | Downloading package wordnet_ic to /home/u1166466/nltk_data...\n",
      "       |   Package wordnet_ic is already up-to-date!\n",
      "       | Downloading package words to /home/u1166466/nltk_data...\n",
      "       |   Package words is already up-to-date!\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package maxent_ne_chunker is already up-to-date!\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package universal_tagset is already up-to-date!\n",
      "       | Downloading package punkt to /home/u1166466/nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package book_grammars to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package book_grammars is already up-to-date!\n",
      "       | Downloading package city_database to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package city_database is already up-to-date!\n",
      "       | Downloading package tagsets to /home/u1166466/nltk_data...\n",
      "       |   Package tagsets is already up-to-date!\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package panlex_swadesh is already up-to-date!\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/u1166466/nltk_data...\n",
      "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
      "       | \n",
      "     Done downloading collection book\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip this cell if you already have executed it in a previous notebook\n",
    "import nltk #NLTK: natural language tool kit\n",
    "nltk.download() #enter 'd' at 'Downloader>' prompt; then enter 'book' at 'Identifier>' prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize       #imports a sentence splitter\n",
    "from nltk.tokenize import word_tokenize       #imports a string tokenizer breaking on whitespace\n",
    "from nltk.tokenize import wordpunct_tokenize  #imports a string tokenizer breaking on whitespace and punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer #used for generating word spans later\n",
    "from nltk import pos_tag                      #a part-of-speech tagger\n",
    "import re                                     #imports Python's regular expression functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this session we will be using the [MIMIC II database](https://physionet.org/mimic2/demo/) often. It is a collection of ICU data on about 4,000 patients available in the public domain without needing a Data Use Agreement.\n",
    "\n",
    "**Some useful code for reading notes from a database (i.e., a simple *Note Reader Process* in the pipeline model):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql       #imports the Python mysql module\n",
    "import pandas as pd  #imports the Python data analysis library \n",
    "import getpass       #imports the getpass module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's connect to MIMIC II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter MySQL passwd for user jovyan········\n"
     ]
    }
   ],
   "source": [
    "dbconn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for user jovyan\"),db='mimic2')\n",
    "cursor = dbconn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's look at the tables in MIMIC II that we can access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             table_name\n",
      "0      a_chartdurations\n",
      "1         a_iodurations\n",
      "2        a_meddurations\n",
      "3             additives\n",
      "4            admissions\n",
      "5          censusevents\n",
      "6           chartevents\n",
      "7    comorbidity_scores\n",
      "8          d_caregivers\n",
      "9           d_careunits\n",
      "10         d_chartitems\n",
      "11  d_chartitems_detail\n",
      "12         d_codeditems\n",
      "13   d_demographicitems\n",
      "14            d_ioitems\n",
      "15           d_labitems\n",
      "16           d_meditems\n",
      "17     d_parammap_items\n",
      "18           d_patients\n",
      "19            db_schema\n",
      "20           deliveries\n",
      "21   demographic_detail\n",
      "22    demographicevents\n",
      "23            drgevents\n",
      "24                 icd9\n",
      "25         icustay_days\n",
      "26       icustay_detail\n",
      "27        icustayevents\n",
      "28             ioevents\n",
      "29            labevents\n",
      "30            medevents\n",
      "31   microbiologyevents\n",
      "32           noteevents\n",
      "33    parameter_mapping\n",
      "34              poe_med\n",
      "35            poe_order\n",
      "36      procedureevents\n",
      "37       totalbalevents\n"
     ]
    }
   ],
   "source": [
    "tables = pd.read_sql(\"SELECT table_name FROM information_schema.tables where table_schema='mimic2'\", dbconn)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How many notes do we have in MIMIC II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(*)\n",
      "0    171927  notes in the noteevents table\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_sql('SELECT count(*) from noteevents limit 10',dbconn), \" notes in the noteevents table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about the **suggested project use case** of extracting concepts related to peripheral artery disease or PAD, let's see how many notes contain 'PAD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(text)\n",
      "0         2435\n"
     ]
    }
   ],
   "source": [
    "#note that \"text\" is the name of the column in noteevents that holds the actual notes\n",
    "#This query takes about 20 seconds to run, be patient!\n",
    "print(pd.read_sql(\"SELECT count(text) from noteevents WHERE text like '%PAD%' LIMIT 10\",dbconn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Well, that's encouraging! Ok, let's read in 10 notes that contain the string 'PAD'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10 notes from noteevents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#note that \"text\" is the name of the column in noteevents that holds the actual notes\n",
    "num_notes = cursor.execute(\"SELECT text from noteevents WHERE text like '%PAD%' limit 10\")\n",
    "print(\"Read\", num_notes,\"notes from noteevents.\\n\")\n",
    "note_list = []\n",
    "for note in cursor:                   #grab each note from the SELECT results        \n",
    "    note_list.append(str(note))       #add each new note to a list of notes as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple *Sentence Segmenter * in the pipeline model\n",
    "Before we explore the clinical text sentences, let's look at `NLTK` running against proper English sentences. In a pipleine approach to NLP, Stage 1 of the pipeline has grabbed the notes we want. So now it's time to extract the sentences from the notes. As an example copy the Introduction paragraph from Canvas Module 4 Web page and paste it between the single quotes in the code below; then run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual text annotation is an important part of an NLP system development project. It often is the only way you can create something you can test your clinical information extraction system against. In a nutshell: in a small set of notes you have humans manually annotate (highlight) mentions of the concepts that your system is trying to extract automatically. The human annotations comprise a reference standard (sometimes called the \"gold standard\"). You can measure the performance of your system by comparing its output with the output generated by humans.\n",
      "1 Manual text annotation is an important part of an NLP system development project.\n",
      "2 It often is the only way you can create something you can test your clinical information extraction system against.\n",
      "3 In a nutshell: in a small set of notes you have humans manually annotate (highlight) mentions of the concepts that your system is trying to extract automatically.\n",
      "4 The human annotations comprise a reference standard (sometimes called the \"gold standard\").\n",
      "5 You can measure the performance of your system by comparing its output with the output generated by humans.\n"
     ]
    }
   ],
   "source": [
    "# Copy the Introduction paragraph from canvas Module 4 and paste below, replacing \"<replace this>\"\n",
    "sentence = 'Manual text annotation is an important part of an NLP system development project. It often is the only way you can create something you can test your clinical information extraction system against. In a nutshell: in a small set of notes you have humans manually annotate (highlight) mentions of the concepts that your system is trying to extract automatically. The human annotations comprise a reference standard (sometimes called the \"gold standard\"). You can measure the performance of your system by comparing its output with the output generated by humans.'\n",
    "print(sentence)\n",
    "sentences = sent_tokenize(sentence)\n",
    "i = 1\n",
    "for next_sent in sentences:\n",
    "    print (i,next_sent)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. `NLTK` did a perfect job extracting each sentence from the Introduction paragraph and placing them in a simple list of sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple *Sentence Tokenizer and Part of Speech* in the pipeline model\n",
    "The next level of processing is to take sentences and break them up into useful parts. The simplest way to break down a sentence is to find each of its tokens, a series of characters between punctuation. `NLTK' has a couple of simple tokenizer functions: word_tokenize(), which breaks out tokens between whitespace characters like <space> and <tab>; and wordpunct_tokenize, which breaks out tokens using whitespace *and* punctuation. Let's run both on the fourth sentence in the Introduction. Can you spot the difference between the output from these two functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'human',\n",
       " 'annotations',\n",
       " 'comprise',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'standard',\n",
       " '(',\n",
       " 'sometimes',\n",
       " 'called',\n",
       " 'the',\n",
       " '``',\n",
       " 'gold',\n",
       " 'standard',\n",
       " \"''\",\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'human',\n",
       " 'annotations',\n",
       " 'comprise',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'standard',\n",
       " '(',\n",
       " 'sometimes',\n",
       " 'called',\n",
       " 'the',\n",
       " '\"',\n",
       " 'gold',\n",
       " 'standard',\n",
       " '\").']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(sentences[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can start to see how the *sentence tokenizer* component of an NLP pipeline could use tokenizer functions extract tokens from segments and, say, count of how often they appear in a text corpus, or even start to build word vectors used by a downstream pipeline component. Let's do a simple example: build a dictionary that maps each token in the Introduction paragraph to a count of the number of times it appears in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common tokens in the Introduction are:\n",
      "the\t6\n",
      ".\t5\n",
      "you\t4\n",
      "system\t4\n",
      "of\t4\n",
      "your\t3\n",
      "is\t3\n",
      "can\t3\n",
      "a\t3\n",
      "standard\t2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#My version: it counts the occurrence of tokens and then lists the 10 most frequent ones\n",
    "word_count = dict() #creating a dictionary for the word count\n",
    "for sentence in sentences: #iterate through the setences\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for token in tokens: #use for loop thru all tokens\n",
    "        token = token.lower()  # make all the words lower case\n",
    "        word_count[token] = word_count.get(token, 0) + 1  #if token count exists add 1 to it, else set it to 0\n",
    "t = [] #create a list\n",
    "for key, value in word_count.items(): #assign key and value for the items\n",
    "    t.append((value, key))\n",
    "t.sort(reverse=True)\n",
    "print('The most common tokens in the Introduction are:')\n",
    "for freq, word in t[:10]:\n",
    "    print(word, freq, sep='\\t')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real quick Parsing\n",
    "Part-of-speech processing and sentence parsing can can get messy very quickly. Working in those areas necesarily means studying formal language and grammar theory. But our lab showed that about 50% of segments in clinical text are not composed in a proper grammar like standard English **anyway**. So the utility of parsing is of limited use in clinical NLP. The `pos_tag()` below is a quick and dirty way to generate part-of-speech tags that are reasonably good.\n",
    "\n",
    "To get a feel for what parsers can do, navigate to the [Online Stanford Parser](http://nlp.stanford.edu:8080/parser/) and enter a few of the sentences from MIMIC II example below. Try \"HTN, DM,PVD, ADMITTED W/ SOB, CP.\" and \"PT VERY SOMNOLENT AT BEGGING OF SHIFT.\" from sentences[1] **below** (note: the current sentences[] list still holds the Introduction sentences. Note I corrected two misspellings (\"SOMULENT\" and \"BEGING\"). Some sophisticated machine learning techniques use POS and parse results in their modeling, but we don't plan to cover that in this course.\n",
    "\n",
    "Try this simple part-of-speech tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('human', 'ADJ'),\n",
       " ('annotations', 'NOUN'),\n",
       " ('comprise', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('reference', 'NOUN'),\n",
       " ('standard', 'NOUN'),\n",
       " ('(', '.'),\n",
       " ('sometimes', 'ADV'),\n",
       " ('called', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('``', '.'),\n",
       " ('gold', 'ADJ'),\n",
       " ('standard', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " (')', '.'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(sentences[3]),tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PT', 'NOUN'),\n",
       " ('VERY', 'NOUN'),\n",
       " ('SOMNOLENT', 'NOUN'),\n",
       " ('AT', 'NOUN'),\n",
       " ('BEGGING', 'NOUN'),\n",
       " ('OF', 'NOUN'),\n",
       " ('SHIFT', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(\"PT VERY SOMNOLENT AT BEGGING OF SHIFT.\"),tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `tagset` tells the part-of-speech generator to use a simple style for POS:\n",
    "\n",
    "|Tag    |Meaning     |English Examples                               |\n",
    "|------ |:----------:|----------------------------------------------:|\n",
    "|ADJ    |adjective \t |new, good, high, special, big, local           \n",
    "|ADP \t|adposition  |on, of, at, with, by, into, under\n",
    "|ADV \t|adverb \t |really, already, still, early, now\n",
    "|CONJ \t|conjunction |and, or, but, if, while, although\n",
    "|DET \t|determiner, |article \tthe, a, some, most, every, no, which\n",
    "|NOUN \t|noun \t     |year, home, costs, time, Africa\n",
    "|NUM \t|numeral \t |twenty-four, fourth, 1991, 14:24\n",
    "|PRT \t|particle \t |at, on, out, over per, that, up, with\n",
    "|PRON \t|pronoun \t |he, their, her, its, my, I, us\n",
    "|VERB \t|verb \t     |is, say, told, given, playing, would\n",
    "|. \t    |punctuation |. , ; !\n",
    "|X \t    |other \t     |ersatz, esprit, dunno, gr8, univeristy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We won't go into detail now, but note that when dealing with annotated files like in the next course Module, we are interested in the span of words. Usually this is an offset from the first character of a clinical note. For example, the word \"annotation\" occurs in the first sentence of the Introduction starting at character #12 and runs through #22. `NLTK` can help us with tasks like determining word spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual text annotation is an important part of an NLP system development project.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 6),\n",
       " (7, 11),\n",
       " (12, 22),\n",
       " (23, 25),\n",
       " (26, 28),\n",
       " (29, 38),\n",
       " (39, 43),\n",
       " (44, 46),\n",
       " (47, 49),\n",
       " (50, 53),\n",
       " (54, 60),\n",
       " (61, 72),\n",
       " (73, 81)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "list(WhitespaceTokenizer().span_tokenize(sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's return to the real, and messy, world of clinical text. The first note we fetched earlier in note_list, looks like this when printed for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nCCU NURSING PROGRESS NOTES\\nS:\"MY THROAT IS SORE\"\\nO:PT IS A 87YR OLD W/ CRF, SP AV GRAFT  [**9-19**]. HTN, DM,PVD, ADMITTED W/ SOB, CP. PT HAD ISCHEMIC CP, PUMP FAILURE, NO RESPONSE TO ANTIANGINALS, DIURETICS. TO CATH LAB YESTERDAY, LAD, OM AND DIAG STENTED. PCWP UP, IABP PLACED. \\nCV:HEMODYNAMICALLY IMPROVED, IABP 1:1W/ GOOD AUGMENTATION, SYS UNLOADING [**5-30**], DIA UNLOADING 2-15MMHG. MOST RECENT CO/CI 5.3/CI 2.96, SVR 1042. PAD DOWN TO 20. UNABLE TO WEDGE SWAN THIS AM. AT 0500 SWAN OUT IN RV, AND ADVANCED BY DR. [**Last Name (STitle) 88**], POSITION CONFIRMED BY CXR.  PT TOL LOW DOSE LOPRESSOR. PRESENTLY PT OFF IV NTG, IV HEPARIN TITRATED AS PER SS, NOW AT 500UNITS/HR. RIGHT GROIN W/ SM AMT OF OOZING AT SITE. BOTH DP PULSES AUDIBLE W/ [**Last Name (un) 89**], L RADIAL PALP. TOES AND HEELS MOTTLED/ DUSKY, WARM TO TOUCH. \\nCVVHD/ABLE TO PULL 200CC/HR. NOW PULLING 50CC PER/HR. SEE CAREVUE FOR FULL I/O DETAILS. \\nRESP:LUNS COARSE LUL, CLEAR RUL, BOTH BASES W/BRONCHIAL BS. O2 WEANED TO 40% THIS AM ABG 7.32/44/91/24, SAT 97%. PT RECEIVED BICARB 1 AMP LAST EVENING FOR PH OF 7.30 PER RENAL. PEDAL EDEMA NOTED.\\nGI:HYPO , COFFEE GROUND COLORED NG ASPIRATE, GUIAC POS 100CC. NO BM.\\nGU:MIN U/O, WAS BR COLOR NOW MORE STRAW COLOR, REMAINS OLIGURIC. \\nSKIN:DUODERM INTACT ON COCCUX.\\nNEURO:PT VERY SOMULENT AT BEGING OF SHIFT. BUT EASILY AROUSABLE ORIENTED X3, PT MORE AWAKE, THIS AM AND C/O RIGHT LEG PAIN, MORPHINE GIVEN. \\n',)\n"
     ]
    }
   ],
   "source": [
    "print(note_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "So `NLTK` should tokenize these sentences properly, right? Let's try segmenting the first note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ('\\nCCU NURSING PROGRESS NOTES\\nS:\"MY THROAT IS SORE\"\\nO:PT IS A 87YR OLD W/ CRF, SP AV GRAFT  [**9-19**]. \n",
      "\n",
      "1 : HTN, DM,PVD, ADMITTED W/ SOB, CP. \n",
      "\n",
      "2 : PT HAD ISCHEMIC CP, PUMP FAILURE, NO RESPONSE TO ANTIANGINALS, DIURETICS. \n",
      "\n",
      "3 : TO CATH LAB YESTERDAY, LAD, OM AND DIAG STENTED. \n",
      "\n",
      "4 : PCWP UP, IABP PLACED. \n",
      "\n",
      "5 : \\nCV:HEMODYNAMICALLY IMPROVED, IABP 1:1W/ GOOD AUGMENTATION, SYS UNLOADING [**5-30**], DIA UNLOADING 2-15MMHG. \n",
      "\n",
      "6 : MOST RECENT CO/CI 5.3/CI 2.96, SVR 1042. \n",
      "\n",
      "7 : PAD DOWN TO 20. \n",
      "\n",
      "8 : UNABLE TO WEDGE SWAN THIS AM. \n",
      "\n",
      "9 : AT 0500 SWAN OUT IN RV, AND ADVANCED BY DR. [**Last Name (STitle) 88**], POSITION CONFIRMED BY CXR. \n",
      "\n",
      "10 : PT TOL LOW DOSE LOPRESSOR. \n",
      "\n",
      "11 : PRESENTLY PT OFF IV NTG, IV HEPARIN TITRATED AS PER SS, NOW AT 500UNITS/HR. \n",
      "\n",
      "12 : RIGHT GROIN W/ SM AMT OF OOZING AT SITE. \n",
      "\n",
      "13 : BOTH DP PULSES AUDIBLE W/ [**Last Name (un) 89**], L RADIAL PALP. \n",
      "\n",
      "14 : TOES AND HEELS MOTTLED/ DUSKY, WARM TO TOUCH. \n",
      "\n",
      "15 : \\nCVVHD/ABLE TO PULL 200CC/HR. \n",
      "\n",
      "16 : NOW PULLING 50CC PER/HR. \n",
      "\n",
      "17 : SEE CAREVUE FOR FULL I/O DETAILS. \n",
      "\n",
      "18 : \\nRESP:LUNS COARSE LUL, CLEAR RUL, BOTH BASES W/BRONCHIAL BS. \n",
      "\n",
      "19 : O2 WEANED TO 40% THIS AM ABG 7.32/44/91/24, SAT 97%. \n",
      "\n",
      "20 : PT RECEIVED BICARB 1 AMP LAST EVENING FOR PH OF 7.30 PER RENAL. \n",
      "\n",
      "21 : PEDAL EDEMA NOTED.\\nGI:HYPO , COFFEE GROUND COLORED NG ASPIRATE, GUIAC POS 100CC. \n",
      "\n",
      "22 : NO BM.\\nGU:MIN U/O, WAS BR COLOR NOW MORE STRAW COLOR, REMAINS OLIGURIC. \n",
      "\n",
      "23 : \\nSKIN:DUODERM INTACT ON COCCUX.\\nNEURO:PT VERY SOMULENT AT BEGING OF SHIFT. \n",
      "\n",
      "24 : BUT EASILY AROUSABLE ORIENTED X3, PT MORE AWAKE, THIS AM AND C/O RIGHT LEG PAIN, MORPHINE GIVEN. \n",
      "\n",
      "25 : \\n',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(note_list[0])\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    print (i,\":\", sentence, \"\\n\")\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sent_tokenizer` really did a pretty good job. It wasn't confused by the periods that occur in numbers like \"ABG 7.32\" There are some newlines that snuck through,like in \"\\nCVVHD/ABLE TO PULL 200CC/HR.\" How would you strip those out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c4b7cf3a1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Here's a simple way:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# Here's a simple way:\n",
    "s = sentences[15]  \n",
    "print(s.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
